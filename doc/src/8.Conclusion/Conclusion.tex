\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
%
In the last decade, avances in the domain of \acrshort{cnn} have led to more accurate networks. This came at a price of an increase of the number of parameters and computational complexity. To perform an efficient inference of the \acrshort{cnn} model, the platform on which the model is executed needs to possess enough computational resources. For this reason, \acrshort{gpu}s are the dominant platform to execute \acrshort{cnn} thanks to their high throughput and reprogrammability. However, \acrshort{gpu}s are not suited for embedded and mobile applications due to their high energy consumption. A better solution would be to implement \acrshort{cnn} on \acrshort{fpga} which provides better energy efficiency, better performance than \acrshort{cpu}, and better reconfigurability than \acrshort{asic}. However, the downside of \acrshort{fpga}s is their constrained ressource, and the challenge is then to find the mapping of large \acrshort{cnn} model to the \acrshort{fpga}.

In the frame of these thesis we looked at different solutions to reduce the size of the model. Two promising optimizations to reduce the size of the \acrshort{cnn} retained our interest : pruning and model optimizations, in particular the use of depthwise separable convolution. The goal of this work is then to combine both approaches by designing an efficient \acrshort{fpga}-based accelerator architecture that integrates sparse \acrshort{dsc} in a \acrshort{cnn} targetting the embedded space.

Before starting the design phase of a structured pruned scheme of the \acrshort{dsc} and the \acrshort{fpga}-based accelerator, we have defined several design objectives that corresponds to the purpose of using pruning:
%
\begin{enumerate}
    \item The pruning scheme is as fine-grained as possible.
    \item The pruning scheme reduces the computational complexity.
    \item The pruning scheme allows a reduction of the memory required to store the weights.
    \item The proposed architecture provides a logically correct output.
    \item An increase of the sparsity improves the performance of the architecture.
\end{enumerate}
%
First, we designed a pruning scheme that prevent the irregular data access patern while imposing the least constraint, satifying the first design objective. The analysis of the reduction factors of the pruning scheme allows us to assess the reduction of the size of the model and its computational complexity.

Second, we analyzed the sparsity created by the pruning scheme to propose a compressed format to reduce the memory utilization depending on the ratio of sparsity. The results showed that, for the quantization chosen (16 bits), we need to prune at least $40 \%$ of the weights to have a reduction of the memory required, and then satifying the third design objective. 

Third, we developed an \acrshort{fpga}-based accelerator that handle the pruning scheme. In the light of the results achieved, the architecture is logically correct and its performance are not degrated with a higher sparsity, satifying the fourth and fifth design objective.

This shows that the proposed five design objectives were met. At the end of this thesis, we can conclude that we have proposed a \acrshort{cnn} optimization to reduce the number of parameters and computational complexity that can be implemented more efficiently on \acrshort{fpga}, compared to \acrshort{gpu} and \acrshort{cpu}. This reduction of weights and computational complexity leads to an acceleration of the inference phase, which was the main goal of this thesis.
%
\section*{Future Works}
\addcontentsline{toc}{section}{Future Works}
%
Multiple paths can be explored to extend this work.

First, since we only designed an accelerator of the inverted residual block of MobileNetV2, we can extend its architecture to support all types of layers of the model and compare it with other implementations to see the real impact of the pruning scheme. Also, several improvements can be applied to the proposed design to accelerate the inference further:
%
\begin{itemize}
    \item We can increase the temporal parallelization (pipelining) of the design by using the ping-pong weight buffer presented in Section \ref{subsec:impl_dsc}. It can be applied when fetching weights from the external memory, but we can also use this structure to pipeline both $1 \times 1$ and depthwise separable convolutions. Indeed, while an intermediate fetching group is fed to the \acrshort{dsc}, the next $1 \times 1$ convolution can be computed. 
    \item In the proposed design, each buffer has one read and write port. We can implement a multi-port buffer by stacking block RAM, each one providing one port. It would allow to reduce the latency of fetching data from the on-chip memory.
\end{itemize}
%
Second, we need to study the impact of the proposed pruning scheme and compare it with channel pruning to determine if there is a significant accuracy gap between the two approaches. Furthemore, we can investigate learning algorithms that conducts the pruning scheme of the $1 \times 1$ kernels. As mentionned in Section \ref{subs:discus}, we also need to consider in the learning phase the fact that we used fixed-point arithmetic instead of floating-point arithmetic, in order to lower the loss of accuracy driven by the integer algorithms.

Finally, based on the pruning scheme, we can extend it to reduce further the number of weights and operations. Indeed, to keep all fetching group to be the same size, we dit not consider the fact that a channel can be pruned in each kernel and that the depthwise and $1 \times 1$ kernels corresponding to that channel can be safely removed. Therefore, a possible extent would be to study if added pruning can be efficiently handled on \acrshort{fpga}.
\afterpage{\blankpage}
\newpage
