\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
%
In the last decade, advances in the domain of \acrshort{cnn} have led to more accurate networks. This came at the price of an increase in the number of parameters and computational complexity. To perform an efficient inference of the \acrshort{cnn} model, the platform on which the model is executed needs to possess enough computational resources. For this reason, \acrshort{gpu}s are the dominant platform to execute \acrshort{cnn} thanks to their high throughput and reprogrammability. However, \acrshort{gpu}s are not suited for embedded and mobile applications due to their high energy consumption. A better solution would be to implement \acrshort{cnn} on \acrshort{fpga} which provides better energy efficiency, better performance than \acrshort{cpu}, and better reconfigurability than \acrshort{asic}. The downside of \acrshort{fpga}s is their constrained resource, and the challenge is then to find the mapping of large \acrshort{cnn} models to the \acrshort{fpga}.

In the frame of this thesis, we looked at different solutions to reduce the size of the model. Two promising optimizations to reduce the size of the \acrshort{cnn} retained our interest: pruning and model optimizations, in particular the use of depthwise separable convolution. The goal of this work was then to combine both approaches by designing an efficient \acrshort{fpga}-based accelerator architecture integrating sparse \acrshort{dsc} in a \acrshort{cnn} targeting the embedded space.

Before starting the design phase of a structured pruned scheme of the \acrshort{dsc} and the \acrshort{fpga}-based accelerator, we defined several design objectives that matches with the purpose of using pruning:
%
\begin{enumerate}
    \item The pruning scheme was as fine-grained as possible.
    \item The pruning scheme reduced computational complexity.
    \item The pruning scheme allowed a reduction of memory required to store the weights.
    \item The proposed architecture provided a logically correct output.
    \item An increase in the sparsity improved the performance of the architecture.
\end{enumerate}
%
First, we designed a pruning scheme that prevented the irregular data access pattern while imposing the least constraints, satisfying the first design objective. As this pruning scheme is finer-grained than kernel and channel-wise pruning, it would not be efficiently implementable on \acrshort{cpu} and \acrshort{gpu}. The analysis of the reduction factors of the pruning scheme allowed us to assess the reduction of the size of the model and its computational complexity.

Second, we analyzed the sparsity created by the pruning scheme to propose a compressed format reducing memory utilization depending on the ratio of sparsity. The results showed that, for the quantization chosen (16 bits), we needed to prune at least $40 \%$ of the weights to have a reduction of memory required, and then satisfying the third design objective. 

Third, we developed an \acrshort{fpga}-based accelerator that handled the pruning scheme. In the light of the results achieved, the architecture was logically correct and its performance was not degraded with a higher sparsity, satisfying the fourth and fifth design objectives.

This showed that the proposed five design objectives were met. At the end of this thesis, we can conclude that we have proposed a \acrshort{cnn} optimization (\acrshort{dsc} and pruning) decreasing the number of parameters and computational complexity, that can be implemented more efficiently on \acrshort{fpga}, compared to \acrshort{gpu} and \acrshort{cpu}. This reduction of weights and computational complexity lead to an acceleration of the inference phase, which was the main objective of this thesis. As a conclusion, we have accelerated convolutional neural networks on FPGA using depthwise separable convolution and pruning.
%
\section*{Future Works}
\addcontentsline{toc}{section}{Future Works}
%
Multiple paths can be explored to extend this work.

First, since we only designed an accelerator of the Bottleneck residual block of MobileNetV2, we can extend its architecture to support all types of layers of the model and compare it with other implementations to see the real impact of the pruning scheme. Also, several improvements can be applied to the proposed design to accelerate the inference further:
%
\begin{itemize}
    \item We can increase the temporal parallelization (pipelining) of the design by using the ping-pong weight buffer presented in Section \ref{subsec:impl_dsc}. It can be applied when fetching weights from the external memory, but we can also use this structure to pipeline both $1 \times 1$ and depthwise separable convolutions. Indeed, while an intermediate fetching group is fed to the \acrshort{dsc}, the next $1 \times 1$ convolution can be computed. 
    \item In the proposed design, each buffer has one read and write port. We can implement a multi-port buffer by stacking block RAM, each one providing one port. It would allow us to reduce the latency of fetching data from the on-chip memory.
    \item The algorithmic optimizations from Section \ref{subsec:algopti} could be used to accelerate the depthwise convolutions.
    \item An analysis using the roofline model presented in Section \ref{sec:opti_dataflow} could be used to determine the optimal hardware design parameters for the target platform.
\end{itemize}
%
Second, we need to study the impact of the proposed pruning scheme and compare it with channel pruning to determine if there is a significant accuracy gap between the two approaches. Furthermore, we can investigate learning algorithms that conduct the pruning scheme of the $1 \times 1$ kernels. As mentioned in Section \ref{subs:discus}, we also need to consider in the learning phase the fact that we used fixed-point arithmetic instead of floating-point arithmetic, to lower the loss of accuracy driven by the integer algorithms.

Finally, based on the pruning scheme, the pruning scheme could be extended to reduce further the number of weights and operations. Indeed, to keep all fetching groups at the same size, we did not consider the fact that a channel can be pruned in each kernel and that the depthwise and $1 \times 1$ kernels corresponding to that channel can be safely removed. Therefore, a possible extent would be to study if added pruning can be efficiently handled on \acrshort{fpga}.
\newpage
