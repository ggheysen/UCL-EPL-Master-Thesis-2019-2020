\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
%
%In the last decade, avances in the domain of \acrshort{cnn} have led to more accurate networks. However, this come at a price of an increase of the number of parameters and omputational complexity. To perform an efficient inference of the \acrshort{cnn} model, the platform on which the model is executed would require enough computational ressources. For this reason, \acrshort{gpu}s are the dominant platform to execute \acrshort{cnn} thanks to their high throughput and reprogrammability. However, \acrshort{gpu}s are not suited for embedded and mobile applications due to their high energy consumption. A better solution would be to implement \acrshort{cnn} on \acrshort{fpga} which provides better energy efficiency, better performance than \acrshort{cpu}, and better reconfigurability than \acrshort{asic}. However, the downside of \acrshort{fpga}s is their constrained ressource, and the challenge is then to find the mapping of large \acrshort{cnn} model to the \acrshort{fpga}.

%In the frame of these thesis we have looked at different solutions to reduce the size of the model. Two promising optimizations retaind our interest : pruning and model optimizations, in particular depthwise separable convolution. The goal of this work is then to combine both approaches by designing an \acrshort{fpga}-based accelerator architecture that integrates sparse \acrshort{dsc} in a \acrshort{cnn} that targets the embedded space.

I have therefore
%
\section*{Future Works}
\addcontentsline{toc}{section}{Future Works}
%

%
\afterpage{\blankpage}
\newpage
