\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
%
In the last decade, avances in the domain of \acrshort{cnn} have led to more accurate networks. However, this come at a price of an increase of the number of parameters and omputational complexity. To perform an efficient inference of the \acrshort{cnn} model, the platform on which the model is executed would require enough computational ressources. For this reason, \acrshort{gpu}s are the dominant platform to execute \acrshort{cnn} thanks to their high throughput and reprogrammability. However, \acrshort{gpu}s are not suited for embedded and mobile applications due to their high energy consumption. A better solution would be to implement \acrshort{cnn} on \acrshort{fpga} which provides better energy efficiency, better performance than \acrshort{cpu}, and better reconfigurability than \acrshort{asic}. However, the downside of \acrshort{fpga}s is their constrained ressource, and the challenge is then to find the mapping of large \acrshort{cnn} model to the \acrshort{fpga}.

In the frame of these thesis we have looked at different solutions to reduce the size of the model. Two promising optimizations retaind our interest : pruning and model optimizations, in particular the use depthwise separable convolution. The goal of this work is then to combine both approaches by designing an efficient \acrshort{fpga}-based accelerator architecture that integrates sparse \acrshort{dsc} in a \acrshort{cnn} that targets the embedded space.

At the end of this thesis, we can conclude that combining both pruning and depthwise separable convolution provides an efficient way to reduce both number of weights and computational complexity in the light of the results achieved. Moreover, the proposed pruning scheme can be efficiently handled on \acrshort{fpga} without any loss of performance when the sparsity increases. For example, we could use the proposed accelerator on embedded devices such as robot.
%
\section*{Future Works}
\addcontentsline{toc}{section}{Future Works}
%
Multiple paths can be explored to extend this work.

First, since we only designed an accelerator of the inverted residual block of MobileNetV2, we can extend its architecture to support all types of layer of the model and compare it with other implementations to see the real impact of the pruning scheme. Moreover, several improvements can be applied to the proposed design to accelerate further the inference:
%
\begin{itemize}
    \item We can increase the temporal parallelization (pipelining) of the design by using the ping-pong weights buffer presented in Section \ref{subsec:impl_dsc}. It can be applied when fetching weights from the external memory, but we can also this this structure to pipeline both $1 \times 1$ and depthwise separable convolutions. Indeed, while an intermediate fetching group is fed to the \acrshort{dsc}, 
    \item In the proposed design, each buffer has one read and write port. We can implement a mult-port buffer by stacking block RAM, each one providing one port. It would allow to reduce the latency of fetching data from the on-chip memory.
\end{itemize}
%
Second, we need to study the impact of the proposed pruning scheme and compare it with channel pruning to determine if the there is a significant accuracy gap between the two approaches. Furthemore, we can investigate learning algorithms that conducts the pruning scheme of the $1 \times 1$ kernels. As mentionned in Section \ref{subs:discus}, we also need to consider in the learning phase the fact that we used fixed-point arithmetic instead of floating-point arithmetic, in order to lower the loss of accuracy driven by the integer algorithms.

Finally, based on the pruning scheme, we can extend it to reduce further the number of weights and operations. Indeed, to keep all fetching group to be the same size, we dit not consider the fact that a channel can be pruned in each kernel and that the depthwise and $1 \times 1$ kernels corresponding to that channel can be safely removed. Therefore, a possible extent would be to study if added pruning can be efficiently handled on \acrshort{fpga}.
\afterpage{\blankpage}
\newpage
