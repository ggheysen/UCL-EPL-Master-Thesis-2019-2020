
@article{alom_history_2018,
	title = {The {History} {Began} from {AlexNet}: {A} {Comprehensive} {Survey} on {Deep} {Learning} {Approaches}},
	volume = {abs/1803.01164},
	shorttitle = {The {History} {Began} from {AlexNet}},
	url = {http://arxiv.org/abs/1803.01164},
	abstract = {Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].},
	urldate = {2020-05-13},
	journal = {CoRR},
	author = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Christopher and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Van Esesn, Brian C. and Awwal, Abdul A. S. and Asari, Vijayan K.},
	month = sep,
	year = {2018},
	note = {arXiv: 1803.01164},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 39 pages, 46 figures, 3 tables. arXiv admin note: text overlap with arXiv:1408.3264, arXiv:1411.4046},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\ZYZBEGQ5\\Alom et al. - 2018 - The History Began from AlexNet A Comprehensive Su.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\NHYSLL8Q\\1803.html:text/html}
}

@article{wason_deep_2018,
	title = {Deep learning: {Evolution} and expansion},
	volume = {52},
	issn = {13890417},
	shorttitle = {Deep learning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389041717303546},
	doi = {10.1016/j.cogsys.2018.08.023},
	abstract = {This paper historically attempts to map the significant success of deep neural networks in notably varied classification problems and application domains with near human-level performance. The paper also addresses the various doubts surrounding the acceptance of deep learning as a science of future. The manuscript attempts to unveil the hidden capabilities of deep neural networks in enabling machines perform the human way tasks which can be learned through what we call observation and experience.},
	language = {en},
	urldate = {2020-05-13},
	journal = {Cognitive Systems Research},
	author = {Wason, Ritika},
	month = dec,
	year = {2018},
	pages = {701--708}
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	volume = {115},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	language = {en},
	number = {3},
	urldate = {2020-05-13},
	journal = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = dec,
	year = {2015},
	pages = {211--252},
	file = {Texte intégral:C\:\\Users\\Guigui\\Zotero\\storage\\ABECK8LB\\Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf}
}

@article{shawahna_fpga-based_2019,
	title = {{FPGA}-{Based} {Accelerators} of {Deep} {Learning} {Networks} for {Learning} and {Classification}: {A} {Review}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {{FPGA}-{Based} {Accelerators} of {Deep} {Learning} {Networks} for {Learning} and {Classification}},
	doi = {10.1109/ACCESS.2018.2890150},
	abstract = {Due to recent advances in digital technologies, and availability of credible data, an area of artificial intelligence, deep learning, has emerged and has demonstrated its ability and effectiveness in solving complex learning problems not possible before. In particular, convolutional neural networks (CNNs) have demonstrated their effectiveness in the image detection and recognition applications. However, they require intensive CPU operations and memory bandwidth that make general CPUs fail to achieve the desired performance levels. Consequently, hardware accelerators that use application-specific integrated circuits, field-programmable gate arrays (FPGAs), and graphic processing units have been employed to improve the throughput of CNNs. More precisely, FPGAs have been recently adopted for accelerating the implementation of deep learning networks due to their ability to maximize parallelism and their energy efficiency. In this paper, we review the recent existing techniques for accelerating deep learning networks on FPGAs. We highlight the key features employed by the various techniques for improving the acceleration performance. In addition, we provide recommendations for enhancing the utilization of FPGAs for CNNs acceleration. The techniques investigated in this paper represent the recent trends in the FPGA-based accelerators of deep learning networks. Thus, this paper is expected to direct the future advances on efficient hardware accelerators and to be useful for deep learning researchers.},
	journal = {IEEE Access},
	author = {Shawahna, Ahmad and Sait, Sadiq M. and El-Maleh, Aiman},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Acceleration, Convolution, convolutional neural networks (CNNs), Field programmable gate arrays, Hardware, Adaptable architectures, deep learning, Deep learning, dynamic reconfiguration, energy-efficient architecture, field programmable gate arrays (FPGAs), hardware accelerator, machine learning, neural networks, Neural networks, optimization, parallel computer architecture, reconfigurable computing, Throughput},
	pages = {7823--7859},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\IKNRASL5\\Shawahna et al. - 2019 - FPGA-Based Accelerators of Deep Learning Networks .pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\PFC87ULN\\8594633.html:text/html}
}

@article{liu_uniform_2019,
	title = {A {Uniform} {Architecture} {Design} for {Accelerating} {2D} and {3D} {CNNs} on {FPGAs}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2079-9292/8/1/65},
	doi = {10.3390/electronics8010065},
	abstract = {Three-dimensional convolutional neural networks (3D CNNs) have gained popularity in many complicated computer vision applications. Many customized accelerators based on FPGAs are proposed for 2D CNNs, while very few are for 3D CNNs. Three-D CNNs are far more computationally intensive and the design space for 3D CNN acceleration has been further expanded since one more dimension is introduced, making it a big challenge to accelerate 3D CNNs on FPGAs. Motivated by the finding that the computation patterns of 2D and 3D CNNs are very similar, we propose a uniform architecture design for accelerating both 2D and 3D CNNs in this paper. The uniform architecture is based on the idea of mapping convolutions to matrix multiplications. A customized mapping module is developed to generate the feature matrix tilings with no need to store the entire enlarged feature matrix on-chip or off-chip, a splitting strategy is adopted to reconstruct a convolutional layer to adapt to the on-chip memory capacity, and a 2D multiply-and-accumulate (MAC) array is adopted to compute matrix multiplications efficiently. For demonstration, we implement an accelerator prototype with a high-level synthesis (HLS) methodology on a Xilinx VC709 board and test the accelerator on three typical CNN models: AlexNet, VGG16, and C3D. Experimental results show that the accelerator achieves state-of-the-art throughput performance on both 2D and 3D CNNs, with much better energy efficiency than the CPU and GPU.},
	language = {en},
	number = {1},
	urldate = {2020-05-13},
	journal = {Electronics},
	author = {Liu, Zhiqiang and Chow, Paul and Xu, Jinwei and Jiang, Jingfei and Dou, Yong and Zhou, Jie},
	month = jan,
	year = {2019},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {accelerator, FPGA, 2D CNN, 2D MAC array, 3D CNN, HLS, matrix multiplication, uniform architecture},
	pages = {65},
	file = {Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\2DEBA54J\\Liu et al. - 2019 - A Uniform Architecture Design for Accelerating 2D .pdf:application/pdf;Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\68SKUNIS\\htm.html:text/html}
}

@article{rosenblatt_perceptron_1958,
	title = {The {Perceptron}: {A} {Probabilistic} {Model} for {Information} {Storage} and {Organization}},
	volume = {65},
	shorttitle = {The {Perceptron}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf},
	abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus 1 The development of this theory has been carried out at the Cornell Aeronautical Laboratory, Inc., under the sponsorship of the Office of Naval Research, Contract Nonr-2381(00). This article is primarily'an adaptation of material reported in Ref. IS, which constitutes the first full report on the program.},
	language = {English},
	number = {6},
	urldate = {2020-06-15},
	journal = {Psychological review},
	author = {Rosenblatt, Frank F.},
	year = {1958},
	pages = {386--408},
	file = {Citeseer - Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\4SLW8XXQ\\summary.html:text/html;Citeseer - Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\WXDBN2BV\\Brain et Rosenblatt - The Perceptron A Probabilistic Model for Informat.pdf:application/pdf}
}

@article{khan_survey_2020,
	title = {A {Survey} of the {Recent} {Architectures} of {Deep} {Convolutional} {Neural} {Networks}},
	issn = {0269-2821, 1573-7462},
	url = {http://arxiv.org/abs/1901.06032},
	doi = {10.1007/s10462-020-09825-6},
	abstract = {Deep Convolutional Neural Network (CNN) is a special type of Neural Networks, which has shown exemplary performance on several competitions related to Computer Vision and Image Processing. Some of the exciting application areas of CNN include Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing, and Speech Recognition. The powerful learning ability of deep CNN is primarily due to the use of multiple feature extraction stages that can automatically learn representations from the data. The availability of a large amount of data and improvement in the hardware technology has accelerated the research in CNNs, and recently interesting deep CNN architectures have been reported. Several inspiring ideas to bring advancements in CNNs have been explored, such as the use of different activation and loss functions, parameter optimization, regularization, and architectural innovations. However, the significant improvement in the representational capacity of the deep CNN is achieved through architectural innovations. Notably, the ideas of exploiting spatial and channel information, depth and width of architecture, and multi-path information processing have gained substantial attention. Similarly, the idea of using a block of layers as a structural unit is also gaining popularity. This survey thus focuses on the intrinsic taxonomy present in the recently reported deep CNN architectures and, consequently, classifies the recent innovations in CNN architectures into seven different categories. These seven categories are based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the elementary understanding of CNN components, current challenges, and applications of CNN are also provided.},
	language = {en},
	urldate = {2020-06-06},
	journal = {Artificial Intelligence Review},
	author = {Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
	month = apr,
	year = {2020},
	note = {arXiv: 1901.06032},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Number of Pages: 70, Number of Figures: 11, Number of Tables: 11. Artif Intell Rev (2020)},
	file = {1901.06032.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\YNJTSKEN\\1901.06032.pdf:application/pdf}
}

@book{mitchell_machine_1997,
	address = {New York},
	series = {{McGraw}-{Hill} series in computer science},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	language = {en},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {Computer algorithms, Machine learning},
	file = {Mitchell - 1997 - Machine Learning.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\QN6IL4V7\\Mitchell - 1997 - Machine Learning.pdf:application/pdf}
}

@book{russell_artificial_2009,
	address = {United States},
	edition = {3},
	title = {Artificial {Intelligence}: {A} {Modern} {Approach}},
	isbn = {0-13-604259-7},
	shorttitle = {Artificial {Intelligence}},
	url = {https://dl.acm.org/doi/book/10.5555/1671238},
	abstract = {This is the eBook of the printed book and may not include anymedia, website access codes, or print supplements that may comepackaged with the bound book.Artificial Intelligence: A ModernApproach, 3e offers the most comprehensive, up-to-dateintroduction to the theory and practice of artificial intelligence.Number one in its field, this textbook is ideal for one ortwo-semester, undergraduate or graduate-level courses in ArtificialIntelligence.Dr. Peter Norvig, contributing ArtificialIntelligence author and ProfessorSebastian Thrun, a Pearson author are offering a free online course atStanford University on artificial intelligence.According to an article in The New York Times, the course on artificial intelligenceis “one of three being offered experimentally by the Stanfordcomputer science department to extend technology knowledge andskills beyond this elite campus to the entire world.” One ofthe other two courses, an introduction to database software, is being taughtby Pearson author Dr. Jennifer Widom.Artificial Intelligence: A Modern Approach, 3e is availableto purchase as an eText for your Kindle™, NOOK™, andthe iPhone®/iPad®.To learn more about the course on artificial intelligence, visithttp://www.ai-class.com. To read thefull New York Times article, click here.},
	language = {English},
	publisher = {Prentice Hall Press},
	author = {Russell, Peter Norvig Stuart},
	month = dec,
	year = {2009}
}

@inproceedings{arnold_introduction_2011,
	address = {Bruges, Belgium},
	title = {An {Introduction} to {Deep} {Learning}},
	volume = {1},
	shorttitle = {An {Introduction} to {Deep} {Learning}},
	url = {https://hal.archives-ouvertes.fr/hal-01352061},
	abstract = {The deep learning paradigm tackles problems on which shallow architectures (e.g. SVM) are aﬀected by the curse of dimensionality. As part of a two-stage learning scheme involving multiple layers of nonlinear processing a set of statistically robust features is automatically extracted from the data. The present tutorial introducing the ESANN deep learning special session details the state-of-the-art models and summarizes the current understanding of this learning approach which is a reference for many diﬃcult classiﬁcation tasks.},
	language = {en},
	urldate = {2020-06-12},
	booktitle = {Proceedings of the {European} {Symposium} on {Artificial} {Neural} {Networks} ({ESANN})},
	author = {Arnold, Ludovic and Rebecchi, Sébastien and Chevallier, Sylvain and Paugam-Moisy, Hélène},
	year = {2011},
	pages = {477--488},
	file = {Arnold et al. - 2011 - An Introduction to Deep Learning.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\VM42ZWFS\\Arnold et al. - 2011 - An Introduction to Deep Learning.pdf:application/pdf}
}

@article{abdelouahab_accelerating_2018,
	title = {Accelerating {CNN} inference on {FPGAs}: {A} {Survey}},
	shorttitle = {Accelerating {CNN} inference on {FPGAs}},
	url = {http://arxiv.org/abs/1806.01683},
	abstract = {Convolutional Neural Networks (CNNs) are currently adopted to solve an ever greater number of problems, ranging from speech recognition to image classification and segmentation. The large amount of processing required by CNNs calls for dedicated and tailored hardware support methods. Moreover, CNN workloads have a streaming nature, well suited to reconfigurable hardware architectures such as FPGAs. The amount and diversity of research on the subject of CNN FPGA acceleration within the last 3 years demonstrates the tremendous industrial and academic interest. This paper presents a state-of-the-art of CNN inference accelerators over FPGAs. The computational workloads, their parallelism and the involved memory accesses are analyzed. At the level of neurons, optimizations of the convolutional and fully connected layers are explained and the performances of the different methods compared. At the network level, approximate computing and datapath optimization methods are covered and state-of-the-art approaches compared. The methods and tools investigated in this survey represent the recent trends in FPGA CNN inference accelerators and will fuel the future advances on efficient hardware deep learning.},
	urldate = {2020-06-12},
	journal = {arXiv:1806.01683 [cs]},
	author = {Abdelouahab, Kamel and Pelcat, Maxime and Serot, Jocelyn and Berry, François},
	month = may,
	year = {2018},
	note = {arXiv: 1806.01683},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Hardware Architecture, Computer Science - Distributed, Parallel, and Cluster Computing},
	annote = {Comment: Cloning our HAL submission in ArXiv, Technical Report - Universite Clermont Auvergne, January 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\876PLGFW\\Abdelouahab et al. - 2018 - Accelerating CNN inference on FPGAs A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\PTFWQVUY\\1806.html:text/html}
}

@article{zhu_efficient_2020,
	title = {An {Efficient} {Hardware} {Accelerator} for {Structured} {Sparse} {Convolutional} {Neural} {Networks} on {FPGAs}},
	url = {http://arxiv.org/abs/2001.01955},
	abstract = {Deep Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in a wide range of applications. However, deeper CNN models, which are usually computation consuming, are widely required for complex Artificial Intelligence (AI) tasks. Though recent research progress on network compression such as pruning has emerged as a promising direction to mitigate computational burden, existing accelerators are still prevented from completely utilizing the benefits of leveraging sparsity owing to the irregularity caused by pruning. On the other hand, Field-Programmable Gate Arrays (FPGAs) have been regarded as a promising hardware platform for CNN inference acceleration. However, most existing FPGA accelerators focus on dense CNN and cannot address the irregularity problem. In this paper, we propose a sparse wise dataflow to skip the cycles of processing Multiply-and-Accumulates (MACs) with zero weights and exploit data statistics to minimize energy through zeros gating to avoid unnecessary computations. The proposed sparse wise dataflow leads to a low bandwidth requirement and a high data sharing. Then we design an FPGA accelerator containing a Vector Generator Module (VGM) which can match the index between sparse weights and input activations according to the proposed dataflow. Experimental results demonstrate that our implementation can achieve 987 imag/s and 48 imag/s performance for AlexNet and VGG-16 on Xilinx ZCU102, respectively, which provides 1.5x to 6.7x speedup and 2.0x to 6.2x energy-efficiency over previous CNN FPGA accelerators.},
	urldate = {2020-06-14},
	journal = {arXiv:2001.01955 [cs, eess]},
	author = {Zhu, Chaoyang and Huang, Kejie and Yang, Shuyuan and Zhu, Ziqi and Zhang, Hejia and Shen, Haibin},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.01955},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\UAE35WRB\\Zhu et al. - 2020 - An Efficient Hardware Accelerator for Structured S.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\9V9S7RGI\\2001.html:text/html}
}

@article{sze_efficient_2017,
	title = {Efficient {Processing} of {Deep} {Neural} {Networks}: {A} {Tutorial} and {Survey}},
	volume = {105},
	issn = {1558-2256},
	shorttitle = {Efficient {Processing} of {Deep} {Neural} {Networks}},
	doi = {10.1109/JPROC.2017.2761740},
	abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
	number = {12},
	journal = {Proceedings of the IEEE},
	author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
	month = dec,
	year = {2017},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Computer architecture, neural nets, deep learning, machine learning, Neural networks, Machine learning, artificial intelligence, Artificial intelligence, ASIC, Benchmark testing, Biological neural networks, computation cost reduction, computational complexity, computer architecture, convolutional neural networks, Convolutional neural networks, dataflow processing, deep neural networks, DNN hardware designs, DNN hardware implementations, energy efficiency, energy-efficient accelerators, hardware architecture, hardware cost, hardware design changes, hardware platforms, low power, Neurons, spatial architectures, Tutorials, VLSI},
	pages = {2295--2329},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\KH2XTFH7\\8114708.html:text/html;Version soumise:C\:\\Users\\Guigui\\Zotero\\storage\\U9QZ6DCD\\Sze et al. - 2017 - Efficient Processing of Deep Neural Networks A Tu.pdf:application/pdf}
}

@book{winograd_arithmetic_1980,
	address = {Philadelphia},
	series = {{CBMS}-{NSF} regional conference series in applied mathematics},
	title = {Arithmetic {Complexity} of {Computations}},
	isbn = {978-0-89871-163-9 0-89871-163-0},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9781611970364},
	language = {English},
	number = {33},
	urldate = {2020-06-14},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Winograd, Shmuel},
	year = {1980},
	note = {https://doi.org/10.1137/1.9781611970364}
}

@article{lavin_fast_2015,
	title = {Fast {Algorithms} for {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1509.09308},
	abstract = {Deep convolutional neural networks take GPU days of compute time to train on large data sets. Pedestrian detection for self driving cars requires very low latency. Image recognition for mobile phones is constrained by limited processing resources. The success of convolutional neural networks in these situations is limited by how fast we can compute them. Conventional FFT based convolution is fast for large filters, but state of the art convolutional neural networks use small, 3x3 filters. We introduce a new class of fast algorithms for convolutional neural networks using Winograd's minimal filtering algorithms. The algorithms compute minimal complexity convolution over small tiles, which makes them fast with small filters and small batch sizes. We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64.},
	urldate = {2020-06-14},
	journal = {arXiv:1509.09308 [cs]},
	author = {Lavin, Andrew and Gray, Scott},
	month = nov,
	year = {2015},
	note = {arXiv: 1509.09308},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, F.2.1, I.2.6},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\2EHFQVVI\\Lavin et Gray - 2015 - Fast Algorithms for Convolutional Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\CBH9KBUK\\1509.html:text/html}
}

@article{sandler_mobilenetv2_2019,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	url = {http://arxiv.org/abs/1801.04381},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
	urldate = {2020-06-14},
	journal = {arXiv:1801.04381 [cs]},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = mar,
	year = {2019},
	note = {arXiv: 1801.04381},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\LNCAJF9C\\Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\QMYJJDFJ\\1801.html:text/html}
}

@inproceedings{jong_hwan_ko_design_2017,
	title = {Design of an energy-efficient accelerator for training of convolutional neural networks using frequency-domain computation},
	doi = {10.1145/3061639.3062228},
	abstract = {Convolutional neural networks (CNNs) require high computation and memory demand for training. This paper presents the design of a frequency-domain accelerator for energy-efficient CNN training. With Fourier representations of parameters, we replace convolutions with simpler pointwise multiplications. To eliminate the Fourier transforms at every layer, we train the network entirely in the frequency domain using approximate frequency-domain nonlinear operations. We further reduce computation and memory requirements using sinc interpolation and Hermitian symmetry. The accelerator is designed and synthesized in 28nm CMOS, as well as prototyped in an FPGA. The simulation results show that the proposed accelerator significantly reduces training time and energy for a target recognition accuracy.},
	booktitle = {2017 54th {ACM}/{EDAC}/{IEEE} {Design} {Automation} {Conference} ({DAC})},
	author = {Jong Hwan Ko and Mudassar, Burhan and Na, Taesik and Mukhopadhyay, Saibal},
	month = jun,
	year = {2017},
	keywords = {convolutional neural network (CNN), FPGA, convolutional neural networks, approximate frequency-domain nonlinear operations, CMOS, energy-efficient accelerator, energy-efficient CNN training, Fourier representations, Fourier transforms, frequency domain, frequency-domain accelerator, frequency-domain analysis, frequency-domain computation, Hermitian symmetry, interpolation, memory demand, memory requirements, neural chips, pointwise multiplications, sinc interpolation, size 28.0 nm, training},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\H6WAI994\\8060431.html:text/html}
}

@book{w_smith_scientist_1997,
	edition = {1},
	title = {The {Scientist} and {Engineer}'s {Guide} to {Digital} {Signal} {Processing}'s {Table} of {Content}},
	isbn = {0-9660176-3-3},
	url = {https://www.dspguide.com/pdfbook.htm},
	urldate = {2020-06-14},
	publisher = {California Technical Pub},
	author = {W. Smith, Steven},
	year = {1997},
	file = {The Scientist and Engineer's Guide to Digital Signal Processing's Table of Content:C\:\\Users\\Guigui\\Zotero\\storage\\H7S4Y995\\pdfbook.html:text/html}
}

@inproceedings{gokhale_240_2014,
	title = {A 240 {G}-ops/s {Mobile} {Coprocessor} for {Deep} {Neural} {Networks}},
	doi = {10.1109/CVPRW.2014.106},
	abstract = {Deep networks are state-of-the-art models used for understanding the content of images, videos, audio and raw input data. Current computing systems are not able to run deep network models in real-time with low power consumption. In this paper we present nn-X: a scalable, low-power coprocessor for enabling real-time execution of deep neural networks. nn-X is implemented on programmable logic devices and comprises an array of configurable processing elements called collections. These collections perform the most common operations in deep networks: convolution, subsampling and non-linear functions. The nn-X system includes 4 high-speed direct memory access interfaces to DDR3 memory and two ARM Cortex-A9 processors. Each port is capable of a sustained throughput of 950 MB/s in full duplex. nn-X is able to achieve a peak performance of 227 G-ops/s, a measured performance in deep learning applications of up to 200 G-ops/s while consuming less than 4 watts of power. This translates to a performance per power improvement of 10 to 100 times that of conventional mobile and desktop processors.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	author = {Gokhale, Vinayak and Jin, Jonghoon and Dundar, Aysegul and Martini, Berin and Culurciello, Eugenio},
	month = jun,
	year = {2014},
	note = {ISSN: 2160-7516},
	keywords = {Convolution, convolution operation, neural nets, machine learning, convolutional neural networks, deep neural networks, ARM Cortex-A9 processors, Artificial neural networks, Computer vision, configurable processing elements, coprocessors, Coprocessors, DDR3 memory, desktop processors, embedded vision system, hardware acceleration, memory access interface, Memory management, mobile coprocessor, mobile processors, nn-X coprocessor, nonlinear function operation, Performance evaluation, power consumption, Program processors, programmable logic devices, subsampling operation},
	pages = {696--701},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\YFXF2RWP\\6910056.html:text/html}
}

@article{abdelouahab_tactics_2017,
	title = {Tactics to {Directly} {Map} {CNN} graphs on {Embedded} {FPGAs}},
	volume = {9},
	url = {https://hal.archives-ouvertes.fr/hal-01626462},
	doi = {10.1109/LES.2017.2743247},
	abstract = {Deep Convolutional Neural Networks (CNNs) are the state-of-the-art in image classification. Since CNN feed forward propagation involves highly regular parallel computation, it benefits from a significant speed-up when running on fine grain parallel programmable logic devices. As a consequence, several studies have proposed FPGA-based accelerators for CNNs. However, because of the large computationalpower required by CNNs, none of the previous studies has proposed a direct mapping of the CNN onto the physical resources of an FPGA, allocating each processing actor to its own hardware instance.In this paper, we demonstrate the feasibility of the so called direct hardware mapping (DHM) and discuss several tactics we explore to make DHM usable in practice. As a proof of concept, we introduce the HADDOC2 open source tool, that automatically transforms a CNN description into a synthesizable hardware description with platform-independent direct hardware mapping.},
	number = {4},
	urldate = {2020-06-15},
	journal = {IEEE Embedded Systems Letters},
	author = {Abdelouahab, Kamel and Pelcat, Maxime and Sérot, Jocelyn and Bourrasset, Cédric and Berry, François},
	year = {2017},
	note = {Publisher: Institute of Electrical and Electronics Engineers},
	keywords = {FPGA, CNN, Deep Learning, VHDL generator},
	pages = {113 -- 116},
	file = {HAL PDF Full Text:C\:\\Users\\Guigui\\Zotero\\storage\\PG63QE8E\\ABDELOUAHAB et al. - 2017 - Tactics to Directly Map CNN graphs on Embedded FPG.pdf:application/pdf}
}

@inproceedings{lin_li_low_2016,
	title = {Low power design methodology for signal processing systems using lightweight dataflow techniques},
	doi = {10.1109/DASIP.2016.7853801},
	abstract = {Dataflow modeling techniques facilitate many aspects of design exploration and optimization for signal processing systems, such as efficient scheduling, memory management, and task synchronization. The lightweight dataflow (LWDF) programming methodology provides an abstract programming model that supports dataflow-based design and implementation of signal processing hardware and software components and systems. Previous work on LWDF techniques has emphasized their application to DSP software implementation. In this paper, we present new extensions of the LWDF methodology for effective integration with hardware description languages (HDLs), and we apply these extensions to develop efficient methods for low power DSP hardware implementation. Through a case study of a deep neural network application for vehicle classification, we demonstrate our proposed LWDF-based hardware design methodology, and its effectiveness in low power implementation of complex signal processing systems.},
	booktitle = {2016 {Conference} on {Design} and {Architectures} for {Signal} and {Image} {Processing} ({DASIP})},
	author = {Lin Li and Fanni, Tiziana and Viitanen, Timo and Renjie Xie and Palumbo, Francesca and Raffo, Luigi and Huttunen, Heikki and Takala, Jarmo and Bhattacharyya, Shuvra S.},
	month = oct,
	year = {2016},
	keywords = {Hardware, neural nets, complex signal processing systems, data flow analysis, dataflow modeling techniques, dataflow-based design, deep neural network application, Design methodology, Digital signal processing, digital signal processing chips, DSP software implementation, hardware description languages, Hardware design languages, HDLs, lightweight dataflow programming methodology, LWDF programming methodology, Ports (Computers), power DSP hardware implementation, Programming, signal processing, Standards, vehicle classification},
	pages = {82--89},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\KDVMBU3D\\7853801.html:text/html}
}

@article{iandola_squeezenet_2016,
	title = {{SqueezeNet}: {AlexNet}-level accuracy with 50x fewer parameters and {\textless}0.{5MB} model size},
	shorttitle = {{SqueezeNet}},
	url = {http://arxiv.org/abs/1602.07360},
	abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
	urldate = {2020-06-17},
	journal = {arXiv:1602.07360 [cs]},
	author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
	month = nov,
	year = {2016},
	note = {arXiv: 1602.07360},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: In ICLR Format},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\R7FTFBXE\\Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\QZTUFVJH\\1602.html:text/html}
}

@article{han_deep_2016,
	title = {Deep {Compression}: {Compressing} {Deep} {Neural} {Networks} with {Pruning}, {Trained} {Quantization} and {Huffman} {Coding}},
	shorttitle = {Deep {Compression}},
	url = {http://arxiv.org/abs/1510.00149},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
	urldate = {2020-06-17},
	journal = {arXiv:1510.00149 [cs]},
	author = {Han, Song and Mao, Huizi and Dally, William J.},
	month = feb,
	year = {2016},
	note = {arXiv: 1510.00149},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Published as a conference paper at ICLR 2016 (oral)},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\JJ9WLATZ\\Han et al. - 2016 - Deep Compression Compressing Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\D84DRTH5\\1510.html:text/html}
}

@article{zoph_learning_2018,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1707.07012},
	abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
	urldate = {2020-06-17},
	journal = {arXiv:1707.07012 [cs, stat]},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	month = apr,
	year = {2018},
	note = {arXiv: 1707.07012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\FUWH4Q73\\Zoph et al. - 2018 - Learning Transferable Architectures for Scalable I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\VSNL9FGA\\1707.html:text/html}
}

@inproceedings{zhang_shufflenet_2018,
	address = {Salt Lake City, UT},
	title = {{ShuffleNet}: {An} {Extremely} {Efficient} {Convolutional} {Neural} {Network} for {Mobile} {Devices}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{ShuffleNet}},
	url = {https://ieeexplore.ieee.org/document/8578814/},
	doi = {10.1109/CVPR.2018.00716},
	abstract = {We introduce an extremely computation-efﬁcient CNN architecture named ShufﬂeNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shufﬂe, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classiﬁcation and MS COCO object detection demonstrate the superior performance of ShufﬂeNet over other structures, e.g. lower top-1 error (absolute 7.8\%) than recent MobileNet [12] on ImageNet classiﬁcation task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShufﬂeNet achieves ∼13× actual speedup over AlexNet while maintaining comparable accuracy.},
	language = {en},
	urldate = {2020-06-17},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
	month = jun,
	year = {2018},
	pages = {6848--6856},
	file = {Zhang et al. - 2018 - ShuffleNet An Extremely Efficient Convolutional N.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\55EQ86WN\\Zhang et al. - 2018 - ShuffleNet An Extremely Efficient Convolutional N.pdf:application/pdf}
}

@article{qasaimeh_comparing_2019,
	title = {Comparing {Energy} {Efficiency} of {CPU}, {GPU} and {FPGA} {Implementations} for {Vision} {Kernels}},
	url = {http://arxiv.org/abs/1906.11879},
	abstract = {Developing high performance embedded vision applications requires balancing run-time performance with energy constraints. Given the mix of hardware accelerators that exist for embedded computer vision (e.g. multi-core CPUs, GPUs, and FPGAs), and their associated vendor optimized vision libraries, it becomes a challenge for developers to navigate this fragmented solution space. To aid with determining which embedded platform is most suitable for their application, we conduct a comprehensive benchmark of the run-time performance and energy efficiency of a wide range of vision kernels. We discuss rationales for why a given underlying hardware architecture innately performs well or poorly based on the characteristics of a range of vision kernel categories. Specifically, our study is performed for three commonly used HW accelerators for embedded vision applications: ARM57 CPU, Jetson TX2 GPU and ZCU102 FPGA, using their vendor optimized vision libraries: OpenCV, VisionWorks and xfOpenCV. Our results show that the GPU achieves an energy/frame reduction ratio of 1.1-3.2x compared to the others for simple kernels. While for more complicated kernels and complete vision pipelines, the FPGA outperforms the others with energy/frame reduction ratios of 1.2-22.3x. It is also observed that the FPGA performs increasingly better as a vision application's pipeline complexity grows.},
	urldate = {2020-06-17},
	journal = {arXiv:1906.11879 [cs, eess]},
	author = {Qasaimeh, Murad and Denolf, Kristof and Lo, Jack and Vissers, Kees and Zambreno, Joseph and Jones, Phillip H.},
	month = may,
	year = {2019},
	note = {arXiv: 1906.11879},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 8 pages, Design Automation Conference (DAC), The 15th IEEE International Conference on Embedded Software and Systems, 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\CUYP8DAD\\Qasaimeh et al. - 2019 - Comparing Energy Efficiency of CPU, GPU and FPGA I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\WGFMBFSP\\1906.html:text/html}
}

@article{howard_mobilenets_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	url = {http://arxiv.org/abs/1704.04861},
	abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	urldate = {2020-06-17},
	journal = {arXiv:1704.04861 [cs]},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.04861},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\MVV3G2SR\\Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\L7M9CC2U\\1704.html:text/html}
}

@inproceedings{zhang_optimizing_2015,
	address = {Monterey, California, USA},
	title = {Optimizing {FPGA}-based {Accelerator} {Design} for {Deep} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4503-3315-3},
	url = {http://dl.acm.org/citation.cfm?doid=2684746.2689060},
	doi = {10.1145/2684746.2689060},
	abstract = {Convolutional neural network (CNN) has been widely employed for image recognition because it can achieve high accuracy by emulating behavior of optic nerves in living creatures. Recently, rapid growth of modern applications based on deep learning algorithms has further improved research and implementations. Especially, various accelerators for deep CNN have been proposed based on FPGA platform because it has advantages of high performance, reconﬁgurability, and fast development round, etc. Although current FPGA accelerators have demonstrated better performance over generic processors, the accelerator design space has not been well exploited. One critical problem is that the computation throughput may not well match the memory bandwidth provided an FPGA platform. Consequently, existing approaches cannot achieve best performance due to underutilization of either logic resource or memory bandwidth. At the same time, the increasing complexity and scalability of deep learning applications aggravate this problem. In order to overcome this problem, we propose an analytical design scheme using the rooﬂine model. For any solution of a CNN design, we quantitatively analyze its computing throughput and required memory bandwidth using various optimization techniques, such as loop tiling and transformation. Then, with the help of rooﬂine model, we can identify the solution with best performance and lowest FPGA resource requirement. As a case study, we implement a CNN accelerator on a VC707 FPGA board and compare it to previous approaches. Our implementation achieves a peak performance of 61.62 GFLOPS under 100MHz working frequency, which outperform previous approaches signiﬁcantly.},
	language = {en},
	urldate = {2020-06-18},
	booktitle = {Proceedings of the 2015 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays} - {FPGA} '15},
	publisher = {ACM Press},
	author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason},
	year = {2015},
	pages = {161--170},
	file = {Zhang et al. - 2015 - Optimizing FPGA-based Accelerator Design for Deep .pdf:C\:\\Users\\Guigui\\Zotero\\storage\\WJMP6WFZ\\Zhang et al. - 2015 - Optimizing FPGA-based Accelerator Design for Deep .pdf:application/pdf}
}

@techreport{williams_roofline_2009,
	title = {Roofline: {An} {Insightful} {Visual} {Performance} {Model} for {Floating}-{Point} {Programs} and {Multicore} {Architectures}},
	shorttitle = {Roofline},
	url = {http://www.osti.gov/servlets/purl/1407078/},
	abstract = {We propose an easy-to-understand, visual performance model that offers insights to programmers and architects on improving parallel software and hardware for floating point computations.},
	language = {en},
	number = {1407078},
	urldate = {2020-06-18},
	author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
	month = sep,
	year = {2009},
	doi = {10.2172/1407078},
	pages = {1407078},
	file = {Williams et al. - 2009 - Roofline An Insightful Visual Performance Model f.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\5IHDX74R\\Williams et al. - 2009 - Roofline An Insightful Visual Performance Model f.pdf:application/pdf}
}

@article{ma_optimizing_2018,
	title = {Optimizing the {Convolution} {Operation} to {Accelerate} {Deep} {Neural} {Networks} on {FPGA}},
	volume = {26},
	issn = {1557-9999},
	doi = {10.1109/TVLSI.2018.2815603},
	abstract = {As convolution contributes most operations in convolutional neural network (CNN), the convolution acceleration scheme significantly affects the efficiency and performance of a hardware CNN accelerator. Convolution involves multiply and accumulate operations with four levels of loops, which results in a large design space. Prior works either employ limited loop optimization techniques, e.g., loop unrolling, tiling, and interchange, or only tune some of the design variables after the accelerator architecture and dataflow are already fixed. Without fully studying the convolution loop optimization before the hardware design phase, the resulting accelerator can hardly exploit the data reuse and manage data movement efficiently. This paper overcomes these barriers by quantitatively analyzing and optimizing the design objectives (e.g., memory access) of the CNN accelerator based on multiple design variables. Then, we propose a specific dataflow of hardware CNN acceleration to minimize the data communication while maximizing the resource utilization to achieve high performance. The proposed CNN acceleration scheme and architecture are demonstrated by implementing end-to-end CNNs including NiN, VGG-16, and ResNet-50/ResNet-152 for inference. For VGG-16 CNN, the overall throughputs achieve 348 GOPS and 715 GOPS on Intel Stratix V and Arria 10 FPGAs, respectively.},
	number = {7},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	author = {Ma, Yufei and Cao, Yu and Vrudhula, Sarma and Seo, Jae-sun},
	month = jul,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	keywords = {accelerate deep neural networks, Acceleration, accelerator architecture, Accelerator architectures, Computer architecture, convolution, Convolution, convolution acceleration scheme, convolution loop optimization, convolution operation, convolutional neural network, convolutional neural networks (CNNs), data communication, dataflow, design objectives, design space, field programmable gate arrays, Field programmable gate arrays, field-programmable gate array (FPGA), FPGA, Hardware, hardware CNN accelerator, hardware design phase, loop unrolling, multiple design variables, neural nets, neural network hardware, optimisation, Optimization, resource utilization, System-on-chip},
	pages = {1354--1367},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\IGABB4AK\\8330049.html:text/html}
}

@article{chen_eyeriss_2017,
	title = {Eyeriss: {An} {Energy}-{Efficient} {Reconfigurable} {Accelerator} for {Deep} {Convolutional} {Neural} {Networks}},
	volume = {52},
	issn = {0018-9200, 1558-173X},
	shorttitle = {Eyeriss},
	url = {http://ieeexplore.ieee.org/document/7738524/},
	doi = {10.1109/JSSC.2016.2616357},
	abstract = {Eyeriss is an accelerator for state-of-the-art deep convolutional neural networks (CNNs). It optimizes for the energy efﬁciency of the entire system, including the accelerator chip and off-chip DRAM, for various CNN shapes by reconﬁguring the architecture. CNNs are widely used in modern AI systems but also bring challenges on throughput and energy efﬁciency to the underlying hardware. This is because its computation requires a large amount of data, creating signiﬁcant data movement from on-chip and off-chip that is more energyconsuming than computation. Minimizing data movement energy cost for any CNN shape, therefore, is the key to high throughput and energy efﬁciency. Eyeriss achieves these goals by using a proposed processing dataﬂow, called row stationary (RS), on a spatial architecture with 168 processing elements. RS dataﬂow reconﬁgures the computation mapping of a given shape, which optimizes energy efﬁciency by maximally reusing data locally to reduce expensive data movement, such as DRAM accesses. Compression and data gating are also applied to further improve energy efﬁciency. Eyeriss processes the convolutional layers at 35 frames/s and 0.0029 DRAM access/multiply and accumulation (MAC) for AlexNet at 278 mW (batch size N = 4), and 0.7 frames/s and 0.0035 DRAM access/MAC for VGG-16 at 236 mW (N = 3).},
	language = {en},
	number = {1},
	urldate = {2020-06-18},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S. and Sze, Vivienne},
	month = jan,
	year = {2017},
	pages = {127--138},
	file = {Chen et al. - 2017 - Eyeriss An Energy-Efficient Reconfigurable Accele.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\GFAKLFFN\\Chen et al. - 2017 - Eyeriss An Energy-Efficient Reconfigurable Accele.pdf:application/pdf}
}

@phdthesis{joos_de_ter_beerst_accelerating_2019,
	address = {Louvain-la-Neuve},
	title = {Accelerating ternary quantized convolutional neural networks using {OpenCL} for {FPGA}},
	url = {http://hdl.handle.net/2078.1/thesis:17752},
	abstract = {FPGAs balance the reprogammability of CPUs and the performance of ASICs. They seem the perfect
solution to increase the throughput of neural networks. However they must also prove to be highly
competitive in a market dominated by GPUs. To achieve this, we focus on the strength of FPGAs that
cannot be taken advantage of on GPUs. Extreme quantization and sparsity are both not suited for
acceleration on GPU. We use a quantized version of ResNet assembled from uniform building-blocks in
order to achieve better area utilization on FPGA. We manage approximately 2.5\% accuracy loss when
compared to a floating-point model, while introducing highly quantized weights and activations. Our final
network uses ternary weights and 4-bit activations, in addition to shift-based batch normalization. With
the use of OpenCL for high-level synthesis, we implement loop tiling, loop unrolling and loop interchange
to speed up the convolution operation of our streamlined model on the Cyclone V FPGA. Using ternary
weights, we are able to remove multiplications and replace them with simple branching, effectively getting
rid of the need for DSPs. Our final FPGA implementation achieves a latency of 17ms per image using a
quantized residual network.},
	urldate = {2020-06-18},
	school = {Ecole polytechnique de Louvain, Université catholique de Louvain},
	author = {Joos de ter Beerst, Victor and Vanderschueren, Antoine},
	year = {2019},
	note = {Prom: De Vleeschouwer, Christophe ; Legat, Jean-Didier}
}

@article{guo_survey_2018,
	title = {A {Survey} of {FPGA}-{Based} {Neural} {Network} {Accelerator}},
	url = {http://arxiv.org/abs/1712.08934},
	abstract = {Recent researches on neural network have shown significant advantage in machine learning over traditional algorithms based on handcrafted features and models. Neural network is now widely adopted in regions like image, speech and video recognition. But the high computation and storage complexity of neural network inference poses great difficulty on its application. CPU platforms are hard to offer enough computation capacity. GPU platforms are the first choice for neural network process because of its high computation capacity and easy to use development frameworks. On the other hand, FPGA-based neural network inference accelerator is becoming a research topic. With specifically designed hardware, FPGA is the next possible solution to surpass GPU in speed and energy efficiency. Various FPGA-based accelerator designs have been proposed with software and hardware optimization techniques to achieve high speed and energy efficiency. In this paper, we give an overview of previous work on neural network inference accelerators based on FPGA and summarize the main techniques used. An investigation from software to hardware, from circuit level to system level is carried out to complete analysis of FPGA-based neural network inference accelerator design and serves as a guide to future work.},
	urldate = {2020-06-18},
	journal = {arXiv:1712.08934 [cs]},
	author = {Guo, Kaiyuan and Zeng, Shulin and Yu, Jincheng and Wang, Yu and Yang, Huazhong},
	month = dec,
	year = {2018},
	note = {arXiv: 1712.08934},
	keywords = {Computer Science - Hardware Architecture},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\5H2KCKZ5\\Guo et al. - 2018 - A Survey of FPGA-Based Neural Network Accelerator.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\PSX8WZHA\\1712.html:text/html}
}

@article{liu_fpga-based_2019,
	title = {An {FPGA}-{Based} {CNN} {Accelerator} {Integrating} {Depthwise} {Separable} {Convolution}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2079-9292/8/3/281},
	doi = {10.3390/electronics8030281},
	abstract = {The Convolutional Neural Network (CNN) has been used in many fields and has achieved remarkable results, such as image classification, face detection, and speech recognition. Compared to GPU (graphics processing unit) and ASIC, a FPGA (field programmable gate array)-based CNN accelerator has great advantages due to its low power consumption and reconfigurable property. However, FPGA\&rsquo;s extremely limited resources and CNN\&rsquo;s huge amount of parameters and computational complexity pose great challenges to the design. Based on the ZYNQ heterogeneous platform and the coordination of resource and bandwidth issues with the roofline model, the CNN accelerator we designed can accelerate both standard convolution and depthwise separable convolution with a high hardware resource rate. The accelerator can handle network layers of different scales through parameter configuration and maximizes bandwidth and achieves full pipelined by using a data stream interface and ping-pong on-chip cache. The experimental results show that the accelerator designed in this paper can achieve 17.11GOPS for 32bit floating point when it can also accelerate depthwise separable convolution, which has obvious advantages compared with other designs.},
	language = {en},
	number = {3},
	urldate = {2020-06-18},
	journal = {Electronics},
	author = {Liu, Bing and Zou, Danyin and Feng, Lei and Feng, Shou and Fu, Ping and Li, Junbao},
	month = mar,
	year = {2019},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {accelerator, convolutional neural network (CNN), depthwise separable convolution, field programmable gate array (FPGA)},
	pages = {281},
	file = {Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\8FDCSBHC\\Liu et al. - 2019 - An FPGA-Based CNN Accelerator Integrating Depthwis.pdf:application/pdf;Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\45XK5AFB\\281.html:text/html}
}

@article{sheng_quantization-friendly_2018,
	title = {A {Quantization}-{Friendly} {Separable} {Convolution} for {MobileNets}},
	url = {http://arxiv.org/abs/1803.08607},
	doi = {10.1109/EMC2.2018.00011},
	abstract = {As deep learning (DL) is being rapidly pushed to edge computing, researchers invented various ways to make inference computation more efficient on mobile/IoT devices, such as network pruning, parameter compression, and etc. Quantization, as one of the key approaches, can effectively offload GPU, and make it possible to deploy DL on fixed-point pipeline. Unfortunately, not all existing networks design are friendly to quantization. For example, the popular lightweight MobileNetV1, while it successfully reduces parameter size and computation latency with separable convolution, our experiment shows its quantized models have large accuracy gap against its float point models. To resolve this, we analyzed the root cause of quantization loss and proposed a quantization-friendly separable convolution architecture. By evaluating the image classification task on ImageNet2012 dataset, our modified MobileNetV1 model can archive 8-bit inference top-1 accuracy in 68.03\%, almost closed the gap to the float pipeline.},
	urldate = {2020-06-18},
	journal = {2018 1st Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2)},
	author = {Sheng, Tao and Feng, Chen and Zhuo, Shaojie and Zhang, Xiaopeng and Shen, Liang and Aleksic, Mickey},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.08607},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {14--18},
	annote = {Comment: Accepted At THE 1ST WORKSHOP ON ENERGY EFFICIENT MACHINE LEARNING AND COGNITIVE COMPUTING FOR EMBEDDED APPLICATIONS (EMC{\textasciicircum}2 2018)},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\33U3D86U\\Sheng et al. - 2018 - A Quantization-Friendly Separable Convolution for .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\9EVZN9ZR\\1803.html:text/html}
}

@inproceedings{nurvitadhi_can_2017,
	address = {Monterey, California, USA},
	title = {Can {FPGAs} {Beat} {GPUs} in {Accelerating} {Next}-{Generation} {Deep} {Neural} {Networks}?},
	isbn = {978-1-4503-4354-1},
	url = {http://dl.acm.org/citation.cfm?doid=3020078.3021740},
	doi = {10.1145/3020078.3021740},
	abstract = {Current-generation Deep Neural Networks (DNNs), such as AlexNet and VGG, rely heavily on dense floating-point matrix multiplication (GEMM), which maps well to GPUs (regular parallelism, high TFLOP/s). Because of this, GPUs are widely used for accelerating DNNs. Current FPGAs offer superior energy efficiency (Ops/Watt), but they do not offer the performance of today’s GPUs on DNNs. In this paper, we look at upcoming FPGA technology advances, the rapid pace of innovation in DNN algorithms, and consider whether future high-performance FPGAs will outperform GPUs for next-generation DNNs.},
	language = {en},
	urldate = {2020-06-19},
	booktitle = {Proceedings of the 2017 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays} - {FPGA} '17},
	publisher = {ACM Press},
	author = {Nurvitadhi, Eriko and Subhaschandra, Suchit and Boudoukh, Guy and Venkatesh, Ganesh and Sim, Jaewoong and Marr, Debbie and Huang, Randy and Ong Gee Hock, Jason and Liew, Yeong Tat and Srivatsan, Krishnan and Moss, Duncan},
	year = {2017},
	pages = {5--14},
	file = {Nurvitadhi et al. - 2017 - Can FPGAs Beat GPUs in Accelerating Next-Generatio.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\DID4C7MM\\Nurvitadhi et al. - 2017 - Can FPGAs Beat GPUs in Accelerating Next-Generatio.pdf:application/pdf}
}

@article{motamedi_placid_2017,
	title = {{PLACID}: {A} {Platform} for {FPGA}-{Based} {Accelerator} {Creation} for {DCNNs}},
	volume = {13},
	issn = {1551-6857},
	shorttitle = {{PLACID}},
	url = {https://doi.org/10.1145/3131289},
	doi = {10.1145/3131289},
	abstract = {Deep Convolutional Neural Networks (DCNNs) exhibit remarkable performance in a number of pattern recognition and classification tasks. Modern DCNNs involve many millions of parameters and billions of operations. Inference using such DCNNs, if implemented as software running on an embedded processor, results in considerable execution time and energy consumption, which is prohibitive in many mobile applications. Field-programmable gate array (FPGA)-based acceleration of DCNN inference is a promising approach to improve both energy consumption and classification throughput. However, the engineering effort required for development and verification of an optimized FPGA-based architecture is significant. In this article, we present PLACID, an automated PLatform for Accelerator CreatIon for DCNNs. PLACID uses an analytical approach to characterization and exploration of the implementation space. PLACID enables generation of an accelerator with the highest throughput for a given DCNN on a specific target FPGA platform. Subsequently, it generates an RTL level architecture in Verilog, which can be passed onto commercial tools for FPGA implementation. PLACID is fully automated, and reduces the accelerator design time from a few months down to a few hours. Experimental results show that architectures synthesized by PLACID yield 2× higher throughput density than the best competing approach.},
	number = {4},
	urldate = {2020-06-19},
	journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
	author = {Motamedi, Mohammad and Gysel, Philipp and Ghiasi, Soheil},
	month = sep,
	year = {2017},
	keywords = {deep learning, Convolutional neural networks, accelerator design, design automation},
	pages = {62:1--62:21},
	file = {Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\SGGDW2UJ\\Motamedi et al. - 2017 - PLACID A Platform for FPGA-Based Accelerator Crea.pdf:application/pdf}
}

@article{szegedy_going_2014,
	title = {Going {Deeper} with {Convolutions}},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	urldate = {2020-06-19},
	journal = {arXiv:1409.4842 [cs]},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.4842},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\VSL4PAHD\\Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\D7KLEUTS\\1409.html:text/html}
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	number = {4},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989},
	note = {Conference Name: Neural Computation},
	pages = {541--551},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\GZXS2XZY\\6795724.html:text/html}
}

@inproceedings{zhang_frequency_2017,
	address = {Monterey, California, USA},
	series = {{FPGA} '17},
	title = {Frequency {Domain} {Acceleration} of {Convolutional} {Neural} {Networks} on {CPU}-{FPGA} {Shared} {Memory} {System}},
	isbn = {978-1-4503-4354-1},
	url = {https://doi.org/10.1145/3020078.3021727},
	doi = {10.1145/3020078.3021727},
	abstract = {We present a novel mechanism to accelerate state-of-art Convolutional Neural Networks (CNNs) on CPU-FPGA platform with coherent shared memory. First, we exploit Fast Fourier Transform (FFT) and Overlap-and-Add (OaA) to reduce the computational requirements of the convolutional layer. We map the frequency domain algorithms onto a highly-parallel OaA-based 2D convolver design on the FPGA. Then, we propose a novel data layout in shared memory for efficient data communication between the CPU and the FPGA. To reduce the memory access latency and sustain peak performance of the FPGA, our design employs double buffering. To reduce the inter-layer data remapping latency, we exploit concurrent processing on the CPU and the FPGA. Our approach can be applied to any kernel size less than the chosen FFT size with appropriate zero-padding leading to acceleration of a wide range of CNN models. We exploit the data parallelism of OaA-based 2D convolver and task parallelism to scale the overall system performance. By using OaA, the number of floating point operations is reduced by 39.14\% {\textasciitilde}54.10\% for the state-of-art CNNs. We implement VGG16, AlexNet and GoogLeNet on Intel QuickAssist QPI FPGA Platform. These designs sustain 123.48 GFLOPs/sec, 83.00 GFLOPs/sec and 96.60 GFLOPs/sec, respectively. Compared with the state-of-the-art AlexNet implementation, our design achieves 1.35x GFLOPs/sec improvement using 3.33x less multipliers and 1.1x less memory. Compared with the state-of-art VGG16 implementation, our design has 0.66x GFLOPs/sec using 3.48x less multipliers without impacting the classification accuracy. For GoogLeNet implementation, our design achieves 5.56x improvement in performance compared with 16 threads running on a 10 Core Intel Xeon Processor at 2.8 GHz.},
	urldate = {2020-06-19},
	booktitle = {Proceedings of the 2017 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Chi and Prasanna, Viktor},
	month = feb,
	year = {2017},
	keywords = {FPGA, convolutional neural networks, concurrent processing, CPU, discrete fourier transform, double buffering, overlap-and-add, shared memory},
	pages = {35--44},
	file = {Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\4Z4TWDFB\\Zhang et Prasanna - 2017 - Frequency Domain Acceleration of Convolutional Neu.pdf:application/pdf}
}

@inproceedings{aydonat_opencl_2017,
	address = {Monterey, California, USA},
	series = {{FPGA} '17},
	title = {An {OpenCL}™ {Deep} {Learning} {Accelerator} on {Arria} 10},
	isbn = {978-1-4503-4354-1},
	url = {https://doi.org/10.1145/3020078.3021738},
	doi = {10.1145/3020078.3021738},
	abstract = {Convolutional neural nets (CNNs) have become a practical means to perform vision tasks, particularly in the area of image classification. FPGAs are well known to be able to perform convolutions efficiently, however, most recent efforts to run CNNs on FPGAs have shown limited advantages over other devices such as GPUs. Previous approaches on FPGAs have often been memory bound due to the limited external memory bandwidth on the FPGA device. We show a novel architecture written in OpenCL(TM), which we refer to as a Deep Learning Accelerator (DLA), that maximizes data reuse and minimizes external memory bandwidth. Furthermore, we show how we can use the Winograd transform to significantly boost the performance of the FPGA. As a result, when running our DLA on Intel's Arria 10 device we can achieve a performance of 1020 img/s, or 23 img/s/W when running the AlexNet CNN benchmark. This comes to 1382 GFLOPs and is 10x faster with 8.4x more GFLOPS and 5.8x better efficiency than the state-of-the-art on FPGAs. Additionally, 23 img/s/W is competitive against the best publicly known implementation of AlexNet on nVidia's TitanX GPU.},
	urldate = {2020-06-19},
	booktitle = {Proceedings of the 2017 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {Association for Computing Machinery},
	author = {Aydonat, Utku and O'Connell, Shane and Capalija, Davor and Ling, Andrew C. and Chiu, Gordon R.},
	month = feb,
	year = {2017},
	keywords = {convolutional neural networks, deep neural networks},
	pages = {55--64},
	file = {Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\PYPLXGZA\\Aydonat et al. - 2017 - An OpenCL™ Deep Learning Accelerator on Arria 10.pdf:application/pdf}
}

@inproceedings{xiao_exploring_2017,
	address = {Austin TX USA},
	title = {Exploring {Heterogeneous} {Algorithms} for {Accelerating} {Deep} {Convolutional} {Neural} {Networks} on {FPGAs}},
	isbn = {978-1-4503-4927-7},
	url = {https://dl.acm.org/doi/10.1145/3061639.3062244},
	doi = {10.1145/3061639.3062244},
	abstract = {Convolutional neural network (CNN) ﬁnds applications in a variety of computer vision applications ranging from object recognition and detection to scene understanding owing to its exceptional accuracy. There exist different algorithms for CNNs computation. In this paper, we explore conventional convolution algorithm with a faster algorithm using Winograd’s minimal ﬁltering theory for efﬁcient FPGA implementation. Distinct from the conventional convolution algorithm, Winograd algorithm uses less computing resources but puts more pressure on the memory bandwidth. We ﬁrst propose a fusion architecture that can fuse multiple layers naturally in CNNs, reusing the intermediate data. Based on this fusion architecture, we explore heterogeneous algorithms to maximize the throughput of a CNN. We design an optimal algorithm to determine the fusion and algorithm strategy for each layer. We also develop an automated toolchain to ease the mapping from Caffe model to FPGA bitstream using Vivado HLS. Experiments using widely used VGG and AlexNet demonstrate that our design achieves up to 1.99X performance speedup compared to the prior fusion-based FPGA accelerator for CNNs.},
	language = {en},
	urldate = {2020-06-20},
	booktitle = {Proceedings of the 54th {Annual} {Design} {Automation} {Conference} 2017},
	publisher = {ACM},
	author = {Xiao, Qingcheng and Liang, Yun and Lu, Liqiang and Yan, Shengen and Tai, Yu-Wing},
	month = jun,
	year = {2017},
	pages = {1--6},
	file = {Xiao et al. - 2017 - Exploring Heterogeneous Algorithms for Acceleratin.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\MHIEFAN8\\Xiao et al. - 2017 - Exploring Heterogeneous Algorithms for Acceleratin.pdf:application/pdf}
}

@inproceedings{podili_fast_2017,
	title = {Fast and efficient implementation of {Convolutional} {Neural} {Networks} on {FPGA}},
	doi = {10.1109/ASAP.2017.7995253},
	abstract = {State-of-the-art CNN models for Image recognition use deep networks with small filters instead of shallow networks with large filters, because the former requires fewer weights. In the light of above trend, we present a fast and efficient FPGA based convolution engine to accelerate CNN models over small filters. The convolution engine implements Winograd minimal filtering algorithm to reduce the number of multiplications by 38\% to 55\% for state-of-the-art CNNs. We exploit the parallelism of the Winograd convolution engine to scale the overall performance. We show that our overall design sustains the peak throughput of the convolution engines. We propose a novel data layout to reduce the required memory bandwidth of our design by half. One noteworthy feature of our Winograd convolution engine is that it hides the computation latency of the pooling layer. As a case study we implement VGG16 CNN model and compare it with previous approaches. Compared with the state-of-the-art reduced precision VGG16 implementation, our implementation achieves 1.2× improvement in throughput by using 3× less multipliers and 2× less on-chip memory without impacting the classification accuracy. The improvements in throughput per multiplier and throughput per unit on-chip memory are 3.7× and 2.47× respectively, compared with the state-of-the-art design.},
	booktitle = {2017 {IEEE} 28th {International} {Conference} on {Application}-specific {Systems}, {Architectures} and {Processors} ({ASAP})},
	author = {Podili, Abhinav and Zhang, Chi and Prasanna, Viktor},
	month = jul,
	year = {2017},
	note = {ISSN: 2160-052X},
	keywords = {convolution, Convolution, field programmable gate arrays, Field programmable gate arrays, FPGA, neural nets, System-on-chip, CNN, Throughput, convolutional neural networks, Convolutional neural networks, Complexity theory, Data reuse, deep networks, Double buffering, Efficient, electronic engineering computing, Engines, image classification, image filtering, image recognition, Kernel, large filters, memory bandwidth, on-chip memory, Pipelining, small filters, storage management chips, Winograd convolution engine, Winograd minimal filtering algorithm},
	pages = {11--18},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\LBDVTYXM\\7995253.html:text/html}
}

@article{cooley_algorithm_1965,
	title = {An algorithm for the machine calculation of complex {Fourier} series},
	volume = {19},
	issn = {0025-5718, 1088-6842},
	url = {https://www.ams.org/mcom/1965-19-090/S0025-5718-1965-0178586-1/},
	doi = {10.1090/S0025-5718-1965-0178586-1},
	abstract = {Advancing research. Creating connections.},
	language = {en},
	number = {90},
	urldate = {2020-06-21},
	journal = {Mathematics of Computation},
	author = {Cooley, James W. and Tukey, John W.},
	year = {1965},
	pages = {297--301},
	file = {Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\VZ23247G\\Cooley et Tukey - 1965 - An algorithm for the machine calculation of comple.pdf:application/pdf;Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\G49IL72D\\S0025-5718-1965-0178586-1.html:text/html}
}

@article{liang_evaluating_2020,
	title = {Evaluating {Fast} {Algorithms} for {Convolutional} {Neural} {Networks} on {FPGAs}},
	volume = {39},
	issn = {1937-4151},
	doi = {10.1109/TCAD.2019.2897701},
	abstract = {In recent years, convolutional neural networks (CNNs) have become widely adopted for computer vision tasks. Field-programmable gate arrays (FPGAs) have been adequately explored as a promising hardware accelerator for CNNs due to its high performance, energy efficiency, and reconfigurability. However, prior FPGA solutions based on the conventional convolutional algorithm is often bounded by the computational capability of FPGAs (e.g., the number of DSPs). To address this problem, the feature maps are transformed to a special domain using fast algorithms to reduce the arithmetic complexity. Winograd and fast Fourier transformation (FFT), as fast algorithm representatives, first transform input data and filter to Winograd or frequency domain, then perform element-wise multiplication, and apply inverse transformation to get the final output. In this paper, we propose a novel architecture for implementing fast algorithms on FPGAs. Our design employs line buffer structure to effectively reuse the feature map data among different tiles. We also effectively pipeline the Winograd/FFT processing element (PE) engine and initiate multiple PEs through parallelization. Meanwhile, there exists a complex design space to explore. We propose an analytical model to predict the resource usage and the performance. Then, we use the model to guide a fast design space exploration. Experiments using the state-of-the-art CNNs demonstrate the best performance and energy efficiency on FPGAs. We achieve 854.6 and 2479.6 GOP/s for AlexNet and VGG16 on Xilinx ZCU102 platform using Winograd. We achieve 130.4 GOP/s for Resnet using Winograd and 201.1 GOP/s for YOLO using FFT on Xilinx ZC706 platform.},
	number = {4},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Liang, Yun and Lu, Liqiang and Xiao, Qingcheng and Yan, Shengen},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {Convolution, field programmable gate arrays, Field programmable gate arrays, field-programmable gate array (FPGA), hardware accelerator, convolutional neural networks, Convolutional neural networks, energy efficiency, electronic engineering computing, AlexNet, Analytical models, arithmetic complexity, complex design space, computer vision tasks, convolutional neural nets, Convolutional neural network (CNN), design space exploration, element-wise multiplication, fast algorithm, fast Fourier transformation, fast Fourier transformation (FFT), fast Fourier transforms, feature map data, field-programmable gate arrays, FPGA solutions, inverse transformation, Prediction algorithms, Space exploration, Transforms, VGG16, Winograd, Winograd-FFT processing element, Xilinx ZC706 platform, Xilinx ZCU102 platform},
	pages = {857--870},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\PSRX3B7X\\8634913.html:text/html}
}

@inproceedings{ahmad_towards_2019,
	title = {Towards {Design} {Space} {Exploration} and {Optimization} of {Fast} {Algorithms} for {Convolutional} {Neural} {Networks} ({CNNs}) on {FPGAs}},
	doi = {10.23919/DATE.2019.8715272},
	abstract = {Convolutional Neural Networks (CNNs) have gained widespread popularity in the field of computer vision and image processing. Due to huge computational requirements of CNNs, dedicated hardware-based implementations are being explored to improve their performance. Hardware platforms such as Field Programmable Gate Arrays (FPGAs) are widely being used to design parallel architectures for this purpose. In this paper, we analyze Winograd minimal filtering or fast convolution algorithms to reduce the arithmetic complexity of convolutional layers of CNNs. We explore a complex design space to find the sets of parameters that result in improved throughput and power-efficiency. We also design a pipelined and parallel Winograd convolution engine that improves the throughput and power-efficiency while reducing the computational complexity of the overall system. Our proposed designs show up to 4.75× and 1.44× improvements in throughput and power-efficiency, respectively, in comparison to the state-of-the-art design while using approximately 2.67× more multipliers. Furthermore, we obtain savings of up to 53.6\% in logic resources compared with the state-of-the-art implementation.},
	booktitle = {2019 {Design}, {Automation} {Test} in {Europe} {Conference} {Exhibition} ({DATE})},
	author = {Ahmad, Afzal and Pasha, Muhammad Adeel},
	month = mar,
	year = {2019},
	note = {ISSN: 1558-1101},
	keywords = {Convolution, field programmable gate arrays, Field programmable gate arrays, FPGA, Hardware, optimisation, CNN, optimization, Throughput, computational complexity, convolutional neural networks, Complexity theory, Kernel, arithmetic complexity, convolutional neural nets, design space exploration, Transforms, computer vision, hardware-based implementations, image processing, parallel architectures, parallel Winograd convolution engine, pipelined Winograd convolution engine, Winograd minimal filtering},
	pages = {1106--1111},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\TF46V46H\\8715272.html:text/html;Version soumise:C\:\\Users\\Guigui\\Zotero\\storage\\J5NZ3DYP\\Ahmad et Pasha - 2019 - Towards Design Space Exploration and Optimization .pdf:application/pdf}
}

@article{chitsaz_acceleration_2020,
	title = {Acceleration of {Convolutional} {Neural} {Network} {Using} {FFT}-{Based} {Split} {Convolutions}},
	url = {http://arxiv.org/abs/2003.12621},
	abstract = {Convolutional neural networks (CNNs) have a large number of variables and hence suffer from a complexity problem for their implementation. Different methods and techniques have developed to alleviate the problem of CNN's complexity, such as quantization, pruning, etc. Among the different simplification methods, computation in the Fourier domain is regarded as a new paradigm for the acceleration of CNNs. Recent studies on Fast Fourier Transform (FFT) based CNN aiming at simplifying the computations required for FFT. However, there is a lot of space for working on the reduction of the computational complexity of FFT. In this paper, a new method for CNN processing in the FFT domain is proposed, which is based on input splitting. There are problems in the computation of FFT using small kernels in situations such as CNN. Splitting can be considered as an effective solution for such issues aroused by small kernels. Using splitting redundancy, such as overlap-and-add, is reduced and, efficiency is increased. Hardware implementation of the proposed FFT method, as well as different analyses of the complexity, are performed to demonstrate the proper performance of the proposed method.},
	urldate = {2020-06-21},
	journal = {arXiv:2003.12621 [cs]},
	author = {Chitsaz, Kamran and Hajabdollahi, Mohsen and Karimi, Nader and Samavi, Shadrokh and Shirani, Shahram},
	month = apr,
	year = {2020},
	note = {arXiv: 2003.12621},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 5 pages, 4 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\GXEX6RE7\\Chitsaz et al. - 2020 - Acceleration of Convolutional Neural Network Using.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\ZC5SFQQ2\\2003.html:text/html}
}

@article{zeng_optimizing_2017,
	title = {Optimizing {Frequency} {Domain} {Implementation} of {CNNs} on {FPGAs}},
	url = {http://ceng.usc.edu/techreports/2017/Prasanna%20CENG-2017-3.pdf},
	abstract = {Convolutional neural networks (CNNs) have become popular for many machine learning applications. Recently, accelerators either in ASIC or using FPGA have been proposed to achieve low latency for classi cation as well as reduce the energy consumption. However, with increasing variety of deep learning models and proliferation of FPGA devices and families, realizing a high-performance design becomes challenging. In this paper, we propose an algorithmarchitecture co-design methodology based on the computational characteristics of CNN models and the features of underlying hardware to realize high performance designs. To speed up various CNN models, our methodology consists of two levels of optimization for convolution in the frequency domain: (1) At the algorithm level, we reduce the total number of operations. We propose a new technique called Concatenate and Pad (CaP). By applying it with its dual operation Overlap and Add (OaA), we can match various image sizes to any given FFT size. Our hybrid algorithm based on native FFT, OaA-FFT and CaP-OaA-FFT achieves signi cant computation reduction for a wide range of CNNs. (2) At the architecture level, our goal is to maximize the utilization of limited on-chip resources. We develop highly e cient hardware building blocks to support our algorithmic optimizations. We develop a simpli ed performance model which captures the constraints of a given hardware device such as external bandwidth, logic resources and on-chip memory. The performance model leads to a reduced design space which is explored using bounded parameter scan to realize a high performance FPGA design for a CNN. We illustrate our methodology by proposing two FPGA designs for AlexNet using throughput and latency as performance metrics. Experimental results show that the two designs achieve throughput of 163.4 GOPS and latency of 12.4 ms respectively, on the Intel HARP heterogeneous platform. Our designs signi cantly improve the state-of-the-art implementations of AlexNet on FPGAs.},
	language = {en},
	urldate = {2020-06-21},
	author = {Zeng, Hanqing and Chen, Ren and Prasanna, Viktor K},
	year = {2017},
	pages = {12},
	file = {Zeng et al. - Optimizing Frequency Domain Implementation of CNNs.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\H9J5MMI4\\Zeng et al. - Optimizing Frequency Domain Implementation of CNNs.pdf:application/pdf}
}

@inproceedings{farabet_cnp_2009,
	title = {{CNP}: {An} {FPGA}-based processor for {Convolutional} {Networks}},
	shorttitle = {{CNP}},
	doi = {10.1109/FPL.2009.5272559},
	abstract = {Convolutional networks (ConvNets) are biologically inspired hierarchical architectures that can be trained to perform a variety of detection, recognition and segmentation tasks. ConvNets have a feed-forward architecture consisting of multiple linear convolution filters interspersed with pointwise non-linear squashing functions. This paper presents an efficient implementation of ConvNets on a low-end DSP-oriented field programmable gate array (FPGA). The implementation exploits the inherent parallelism of ConvNets and takes full advantage of multiple hardware multiply accumulate units on the FPGA. The entire system uses a single FPGA with an external memory module, and no extra parts. A network compiler software was implemented, which takes a description of a trained ConvNet and compiles it into a sequence of instructions for the ConvNet Processor (CNP). A ConvNet face detection system was implemented and tested. Face detection on a 512 times 384 frame takes 100 ms (10 frames per second), which corresponds to an average performance of 3.4 times 109 connections per second for this 340 million connection network. The design can be used for low-power, lightweight embedded vision systems for micro-UAVs and other small robots.},
	booktitle = {2009 {International} {Conference} on {Field} {Programmable} {Logic} and {Applications}},
	author = {Farabet, Clement and Poulet, Cyril and Han, Jefferson Y. and LeCun, Yann},
	month = aug,
	year = {2009},
	note = {ISSN: 1946-1488},
	keywords = {Convolution, field programmable gate arrays, Field programmable gate arrays, Hardware, digital signal processing chips, Cameras, CNP, ConvNet face detection, ConvNet processor, convolutional network, DSP-oriented field programmable gate array, Face detection, feed-forward architecture, Feedforward systems, Filters, FPGA-based processor, linear convolution filter, Machine vision, Navigation, network compiler software, nonlinear squashing function, Robot vision systems},
	pages = {32--37},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\ECGIY78N\\5272559.html:text/html}
}

@article{mittal_survey_2020,
	title = {A survey of {FPGA}-based accelerators for convolutional neural networks},
	volume = {32},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-018-3761-1},
	doi = {10.1007/s00521-018-3761-1},
	abstract = {Deep convolutional neural networks (CNNs) have recently shown very high accuracy in a wide range of cognitive tasks, and due to this, they have received significant interest from the researchers. Given the high computational demands of CNNs, custom hardware accelerators are vital for boosting their performance. The high energy efficiency, computing capabilities and reconfigurability of FPGA make it a promising platform for hardware acceleration of CNNs. In this paper, we present a survey of techniques for implementing and optimizing CNN algorithms on FPGA. We organize the works in several categories to bring out their similarities and differences. This paper is expected to be useful for researchers in the area of artificial intelligence, hardware architecture and system design.},
	language = {en},
	number = {4},
	urldate = {2020-06-22},
	journal = {Neural Computing and Applications},
	author = {Mittal, Sparsh},
	month = feb,
	year = {2020},
	pages = {1109--1139}
}

@inproceedings{wei_automated_2017,
	address = {Austin TX USA},
	title = {Automated {Systolic} {Array} {Architecture} {Synthesis} for {High} {Throughput} {CNN} {Inference} on {FPGAs}},
	isbn = {978-1-4503-4927-7},
	url = {https://dl.acm.org/doi/10.1145/3061639.3062207},
	doi = {10.1145/3061639.3062207},
	abstract = {Convolutional neural networks (CNNs) have been widely applied in many deep learning applications. In recent years, the FPGA implementation for CNNs has attracted much attention because of its high performance and energy efﬁciency. However, existing implementations have difﬁculty to fully leverage the computation power of the latest FPGAs. In this paper we implement CNN on an FPGA using a systolic array architecture, which can achieve high clock frequency under high resource utilization. We provide an analytical model for performance and resource utilization and develop an automatic design space exploration framework, as well as sourceto-source code transformation from a C program to a CNN implementation using systolic array. The experimental results show that our framework is able to generate the accelerator for real-life CNN models, achieving up to 461 GFlops for ﬂoating point data type and 1.2 Tops for 8-16 bit ﬁxed point.},
	language = {en},
	urldate = {2020-06-22},
	booktitle = {Proceedings of the 54th {Annual} {Design} {Automation} {Conference} 2017},
	publisher = {ACM},
	author = {Wei, Xuechao and Yu, Cody Hao and Zhang, Peng and Chen, Youxiang and Wang, Yuxin and Hu, Han and Liang, Yun and Cong, Jason},
	month = jun,
	year = {2017},
	pages = {1--6},
	file = {Wei et al. - 2017 - Automated Systolic Array Architecture Synthesis fo.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\8QXIWW2Q\\Wei et al. - 2017 - Automated Systolic Array Architecture Synthesis fo.pdf:application/pdf}
}

@inproceedings{venieris_latency-driven_2017,
	title = {Latency-driven design for {FPGA}-based convolutional neural networks},
	doi = {10.23919/FPL.2017.8056828},
	abstract = {In recent years, Convolutional Neural Networks (ConvNets) have become the quintessential component of several state-of-the-art Artificial Intelligence tasks. Across the spectrum of applications, the performance needs vary significantly, from high-throughput image recognition to the very low-latency requirements of autonomous cars. In this context, FPGAs can provide a potential platform that can be optimally configured based on different performance requirements. However, with the increasing complexity of ConvNet models, the architectural design space becomes overwhelmingly large, asking for principled design flows that address the application-level needs. This paper presents a latency-driven design methodology for mapping ConvNets on FPGAs. The proposed design flow employs novel transformations over a Synchronous Dataflow-based modelling framework together with a latency-centric optimisation procedure in order to efficiently explore the design space targeting low-latency designs. Quantitative evaluation shows large improvements in latency when latency-driven optimisation is in place yielding designs that improve the latency of AlexNet by 73.54× and VGG16 by 5.61× over throughput-optimised designs.},
	booktitle = {2017 27th {International} {Conference} on {Field} {Programmable} {Logic} and {Applications} ({FPL})},
	author = {Venieris, Stylianos I. and Bouganis, Christos-Savvas},
	month = sep,
	year = {2017},
	note = {ISSN: 1946-1488},
	keywords = {Convolution, field programmable gate arrays, Field programmable gate arrays, FPGA, neural nets, Machine learning, convolutional neural networks, image recognition, AlexNet, Space exploration, VGG16, architectural design space, Artificial Intelligence tasks, autonomous cars, Biological system modeling, Computational modeling, ConvNet models, ConvNets mapping, design flow, design methodology, Feature extraction, latency-centric optimisation procedure, logic design, low-latency requirements, quantitative evaluation, quintessential component, throughput-optimised designs},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\LQQTCMCR\\8056828.html:text/html}
}

@article{lee_static_1987,
	title = {Static {Scheduling} of {Synchronous} {Data} {Flow} {Programs} for {Digital} {Signal} {Processing}},
	volume = {C-36},
	url = {https://ptolemy.berkeley.edu/publications/papers/87/staticscheduling/},
	abstract = {Large grain data flow (LGDF) programming is natural and convenient for describing digital signal processing (DSP) systems, but its runtime overhead is costly in real time or cost-sensitive applications. In some situations, designers are notwilling to squander computing resources for the sake of programmer convenience. This is particularly true when the target machine is a programmable DSP chip. However, the runtime overhead inherent in most LGDF implementations is not required for most signal processing systems because such systems are mostly synchronous (in the DSP sense). Synchronous data flow (SDF) differs from traditional data flow in that the amount of data produced and consumed by a data flow node is specified a priori for each input and output. This is equivalent to specifying the relative sample rates in signal processing system. This means that the scheduling of SDF nodes need not be done at runtime, but can be done at compile time (statically), so the runtime overhead evaporates. The sample rates can all be different, which is not true of most current data-driven digital signal processing programming methodologies. Synchronous data flow is closely related to computation graphs, a special case of Petri nets. This self-contained paper develops the theory necessary to statically schedule SDF programs on single or multiple processors. A class of static (compile time) scheduling algorithms is proven valid, and specific algorithms are given for scheduling SDF systems onto single or multiple processors.},
	language = {en},
	number = {1},
	urldate = {2020-06-22},
	journal = {IEEE Transactions on Computers},
	author = {Lee, Edward and Messerschmitt, David},
	year = {1987},
	pages = {24--35},
	file = {Lee - Programs for Digital Signal Processing.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\86NTKBPX\\Lee - Programs for Digital Signal Processing.pdf:application/pdf}
}

@article{ma_shufflenet_2018,
	title = {{ShuffleNet} {V2}: {Practical} {Guidelines} for {Efficient} {CNN} {Architecture} {Design}},
	shorttitle = {{ShuffleNet} {V2}},
	url = {https://arxiv.org/abs/1807.11164v1},
	abstract = {Currently, the neural network architecture design is mostly guided by the
{\textbackslash}emph\{indirect\} metric of computation complexity, i.e., FLOPs. However, the
{\textbackslash}emph\{direct\} metric, e.g., speed, also depends on the other factors such as
memory access cost and platform characterics. Thus, this work proposes to
evaluate the direct metric on the target platform, beyond only considering
FLOPs. Based on a series of controlled experiments, this work derives several
practical {\textbackslash}emph\{guidelines\} for efficient network design. Accordingly, a new
architecture is presented, called {\textbackslash}emph\{ShuffleNet V2\}. Comprehensive ablation
experiments verify that our model is the state-of-the-art in terms of speed and
accuracy tradeoff.},
	language = {en},
	urldate = {2020-06-22},
	author = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
	month = jul,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\LYLJRTEL\\Ma et al. - 2018 - ShuffleNet V2 Practical Guidelines for Efficient .pdf:application/pdf;Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\QN9E7B4R\\1807.html:text/html}
}

@article{cheng_recent_2018,
	title = {Recent {Advances} in {Efficient} {Computation} of {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1802.00939},
	abstract = {Deep neural networks have evolved remarkably over the past few years and they are currently the fundamental tools of many intelligent systems. At the same time, the computational complexity and resource consumption of these networks also continue to increase. This will pose a significant challenge to the deployment of such networks, especially in real-time applications or on resource-limited devices. Thus, network acceleration has become a hot topic within the deep learning community. As for hardware implementation of deep neural networks, a batch of accelerators based on FPGA/ASIC have been proposed in recent years. In this paper, we provide a comprehensive survey of recent advances in network acceleration, compression and accelerator design from both algorithm and hardware points of view. Specifically, we provide a thorough analysis of each of the following topics: network pruning, low-rank approximation, network quantization, teacher-student networks, compact network design and hardware accelerators. Finally, we will introduce and discuss a few possible future directions.},
	urldate = {2020-06-23},
	journal = {arXiv:1802.00939 [cs]},
	author = {Cheng, Jian and Wang, Peisong and Li, Gang and Hu, Qinghao and Lu, Hanqing},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.00939},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 14 pages, 3 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\P9MIZHZC\\Cheng et al. - 2018 - Recent Advances in Efficient Computation of Deep C.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\245V3VTW\\1802.html:text/html}
}

@inproceedings{huimin_li_high_2016,
	title = {A high performance {FPGA}-based accelerator for large-scale convolutional neural networks},
	doi = {10.1109/FPL.2016.7577308},
	abstract = {In recent years, convolutional neural networks (CNNs) based machine learning algorithms have been widely applied in computer vision applications. However, for large-scale CNNs, the computation-intensive, memory-intensive and resource-consuming features have brought many challenges to CNN implementations. This work proposes an end-to-end FPGA-based CNN accelerator with all the layers mapped on one chip so that different layers can work concurrently in a pipelined structure to increase the throughput. A methodology which can find the optimized parallelism strategy for each layer is proposed to achieve high throughput and high resource utilization. In addition, a batch-based computing method is implemented and applied on fully connected layers (FC layers) to increase the memory bandwidth utilization due to the memory-intensive feature. Further, by applying two different computing patterns on FC layers, the required on-chip buffers can be reduced significantly. As a case study, a state-of-the-art large-scale CNN, AlexNet, is implemented on Xilinx VC709. It can achieve a peak performance of 565.94 GOP/s and 391 FPS under 156MHz clock frequency which outperforms previous approaches.},
	booktitle = {2016 26th {International} {Conference} on {Field} {Programmable} {Logic} and {Applications} ({FPL})},
	author = {Huimin Li and Xitian Fan and Li Jiao and Wei Cao and Xuegong Zhou and Lingli Wang},
	month = aug,
	year = {2016},
	note = {ISSN: 1946-1488},
	keywords = {Convolution, field programmable gate arrays, neural nets, resource utilization, System-on-chip, Throughput, convolutional neural networks, Neurons, memory bandwidth, AlexNet, computer vision, Computational modeling, Bandwidth, batch-based computing method, computation-intensive features, computer vision applications, FC layers, FPGA-based CNN accelerator, fully connected layers, high performance FPGA-based accelerator, learning (artificial intelligence), machine learning algorithms, memory bandwidth utilization, memory-intensive features, on-chip buffers, optimized parallelism strategy, Parallel processing, parallelism, pipeline, resource-consuming features, Xilinx VC709},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\L2Y4X69D\\7577308.html:text/html}
}

@article{bai_cnn_2018,
	title = {A {CNN} {Accelerator} on {FPGA} {Using} {Depthwise} {Separable} {Convolution}},
	volume = {65},
	issn = {1558-3791},
	doi = {10.1109/TCSII.2018.2865896},
	abstract = {Convolutional neural networks (CNNs) have been widely deployed in the fields of computer vision and pattern recognition because of their high accuracy. However, large convolution operations are computing intensive and often require a powerful computing platform such as a graphics processing unit. This makes it difficult to apply CNNs to portable devices. The state-of-the-art CNNs, such as MobileNetV2 and Xception, adopt depthwise separable convolution to replace the standard convolution for embedded platforms, which significantly reduces operations and parameters with only limited loss in accuracy. This highly structured model is very suitable for field-programmable gate array (FPGA) implementation. In this brief, a scalable high performance depthwise separable convolution optimized CNN accelerator is proposed. The accelerator can be fit into an FPGA of different sizes, provided the balancing between hardware resources and processing speed. As an example, MobileNetV2 is implemented on Arria 10 SoC FPGA, and the results show this accelerator can classify each picture from ImageNet in 3.75 ms, which is about 266.6 frames per second. The FPGA design achieves 20× speedup if compared to CPU.},
	number = {10},
	journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	author = {Bai, Lin and Zhao, Yiming and Huang, Xinming},
	month = oct,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Circuits and Systems II: Express Briefs},
	keywords = {convolution, Convolution, field programmable gate arrays, Field programmable gate arrays, FPGA, neural nets, System-on-chip, hardware accelerator, MobileNetV2, convolutional neural networks, Standards, Engines, computer vision, Bandwidth, Adders, Arria 10 SoC FPGA, CNN accelerator, convolution operations, Convolutional neural network, field-programmable gate array implementation, FPGA design, graphics processing unit, highly structured model, pattern recognition, powerful computing platform, scalable high performance depthwise separable convolution, standard convolution, system-on-chip},
	pages = {1415--1419},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\5DCFGVHT\\8438987.html:text/html;Version soumise:C\:\\Users\\Guigui\\Zotero\\storage\\TMB9W4LM\\Bai et al. - 2018 - A CNN Accelerator on FPGA Using Depthwise Separabl.pdf:application/pdf}
}

@article{courbariaux_binarized_2016,
	title = {Binarized {Neural} {Networks}: {Training} {Deep} {Neural} {Networks} with {Weights} and {Activations} {Constrained} to +1 or -1},
	shorttitle = {Binarized {Neural} {Networks}},
	url = {http://arxiv.org/abs/1602.02830},
	abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
	urldate = {2020-06-23},
	journal = {arXiv:1602.02830 [cs]},
	author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
	month = mar,
	year = {2016},
	note = {arXiv: 1602.02830},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 11 pages and 3 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\ERLL3LBG\\Courbariaux et al. - 2016 - Binarized Neural Networks Training Deep Neural Ne.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\TCU8EF9J\\1602.html:text/html}
}

@article{li_ternary_2016,
	title = {Ternary {Weight} {Networks}},
	url = {http://arxiv.org/abs/1605.04711},
	abstract = {We introduce ternary weight networks (TWNs) - neural networks with weights constrained to +1, 0 and -1. The Euclidian distance between full (float or double) precision weights and the ternary weights along with a scaling factor is minimized. Besides, a threshold-based ternary function is optimized to get an approximated solution which can be fast and easily computed. TWNs have stronger expressive abilities than the recently proposed binary precision counterparts and are thus more effective than the latter. Meanwhile, TWNs achieve up to 16\${\textbackslash}times\$ or 32\${\textbackslash}times\$ model compression rate and need fewer multiplications compared with the full precision counterparts. Benchmarks on MNIST, CIFAR-10, and large scale ImageNet datasets show that the performance of TWNs is only slightly worse than the full precision counterparts but outperforms the analogous binary precision counterparts a lot.},
	urldate = {2020-06-23},
	journal = {arXiv:1605.04711 [cs]},
	author = {Li, Fengfu and Zhang, Bo and Liu, Bin},
	month = nov,
	year = {2016},
	note = {arXiv: 1605.04711},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 5 pages, 3 fitures, conference},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\8SGHYAFZ\\Li et al. - 2016 - Ternary Weight Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\TMAW7UF8\\1605.html:text/html}
}

@inproceedings{qiu_going_2016,
	address = {Monterey, California, USA},
	series = {{FPGA} '16},
	title = {Going {Deeper} with {Embedded} {FPGA} {Platform} for {Convolutional} {Neural} {Network}},
	isbn = {978-1-4503-3856-1},
	url = {https://doi.org/10.1145/2847263.2847265},
	doi = {10.1145/2847263.2847265},
	abstract = {In recent years, convolutional neural network (CNN) based methods have achieved great success in a large number of applications and have been among the most powerful and widely used techniques in computer vision. However, CNN-based methods are com-putational-intensive and resource-consuming, and thus are hard to be integrated into embedded systems such as smart phones, smart glasses, and robots. FPGA is one of the most promising platforms for accelerating CNN, but the limited bandwidth and on-chip memory size limit the performance of FPGA accelerator for CNN. In this paper, we go deeper with the embedded FPGA platform on accelerating CNNs and propose a CNN accelerator design on embedded FPGA for Image-Net large-scale image classification. We first present an in-depth analysis of state-of-the-art CNN models and show that Convolutional layers are computational-centric and Fully-Connected layers are memory-centric. Then the dynamic-precision data quantization method and a convolver design that is efficient for all layer types in CNN are proposed to improve the bandwidth and resource utilization. Results show that only 0.4\% accuracy loss is introduced by our data quantization flow for the very deep VGG16 model when 8/4-bit quantization is used. A data arrangement method is proposed to further ensure a high utilization of the external memory bandwidth. Finally, a state-of-the-art CNN, VGG16-SVD, is implemented on an embedded FPGA platform as a case study. VGG16-SVD is the largest and most accurate network that has been implemented on FPGA end-to-end so far. The system on Xilinx Zynq ZC706 board achieves a frame rate at 4.45 fps with the top-5 accuracy of 86.66\% using 16-bit quantization. The average performance of convolutional layers and the full CNN is 187.8 GOP/s and 137.0 GOP/s under 150MHz working frequency, which outperform previous approaches significantly.},
	urldate = {2020-06-23},
	booktitle = {Proceedings of the 2016 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays}},
	publisher = {Association for Computing Machinery},
	author = {Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen and Wang, Yu and Yang, Huazhong},
	month = feb,
	year = {2016},
	keywords = {bandwidth utilization, convolutional neural network (cnn), dynamic-precision data quantization, embedded fpga},
	pages = {26--35},
	file = {Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\UI9HFYUP\\Qiu et al. - 2016 - Going Deeper with Embedded FPGA Platform for Convo.pdf:application/pdf}
}

@article{yin_high_2018,
	title = {A {High} {Energy} {Efficient} {Reconfigurable} {Hybrid} {Neural} {Network} {Processor} for {Deep} {Learning} {Applications}},
	volume = {53},
	issn = {1558-173X},
	doi = {10.1109/JSSC.2017.2778281},
	abstract = {Hybrid neural networks (hybrid-NNs) have been widely used and brought new challenges to NN processors. Thinker is an energy efficient reconfigurable hybrid-NN processor fabricated in 65-nm technology. To achieve high energy efficiency, three optimization techniques are proposed. First, each processing element (PE) supports bit-width adaptive computing to meet various bit-widths of neural layers, which raises computing throughput by 91\% and improves energy efficiency by 1.93× on average. Second, PE array supports on-demand array partitioning and reconfiguration for processing different NNs in parallel, which results in 13.7\% improvement of PE utilization and improves energy efficiency by 1.11×. Third, a fused data pattern-based multi-bank memory system is designed to exploit data reuse and guarantee parallel data access, which improves computing throughput and energy efficiency by 1.11× and 1.17×, respectively. Measurement results show that this processor achieves 5.09-TOPS/W energy efficiency at most.},
	number = {4},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Yin, Shouyi and Ouyang, Peng and Tang, Shibin and Tu, Fengbin and Li, Xiudong and Zheng, Shixuan and Lu, Tianyi and Gu, Jiangyuan and Liu, Leibo and Wei, Shaojun},
	month = apr,
	year = {2018},
	note = {Conference Name: IEEE Journal of Solid-State Circuits},
	keywords = {Acceleration, neural nets, reconfigurable computing, Throughput, Artificial neural networks, coprocessors, parallel architectures, logic design, learning (artificial intelligence), Arrays, bit-width adaptive computing, computing throughput, deep learning applications, Energy efficiency, fused data pattern-based multibank memory system, high energy efficient reconfigurable hybrid neural network processor, hybrid neural networks (hybrid-NNs), hybrid-NN, memory architecture, memory banking, microprocessor chips, neural layers, NN processors, on-demand array partitioning, parallel data access, reconfigurable architectures, resource partitioning, Speech recognition},
	pages = {968--982},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\5Q8VL5TE\\8207783.html:text/html}
}

@article{liu_rethinking_2019,
	title = {Rethinking the {Value} of {Network} {Pruning}},
	url = {http://arxiv.org/abs/1810.05270},
	abstract = {Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned "important" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited "important" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the "Lottery Ticket Hypothesis" (Frankle \& Carbin 2019), and find that with optimal learning rate, the "winning ticket" initialization as used in Frankle \& Carbin (2019) does not bring improvement over random initialization.},
	urldate = {2020-06-24},
	journal = {arXiv:1810.05270 [cs, stat]},
	author = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
	month = mar,
	year = {2019},
	note = {arXiv: 1810.05270},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2019. Significant revisions from the previous version},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\8LH6SQRZ\\Liu et al. - 2019 - Rethinking the Value of Network Pruning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\PCIGEGFJ\\1810.html:text/html}
}

@article{frankle_lottery_2019,
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Sparse}, {Trainable} {Neural} {Networks}},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1803.03635},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	urldate = {2020-06-24},
	journal = {arXiv:1803.03635 [cs]},
	author = {Frankle, Jonathan and Carbin, Michael},
	month = mar,
	year = {2019},
	note = {arXiv: 1803.03635},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: ICLR camera ready},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\FB7JEUHZ\\Frankle et Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\ZV48XCQA\\1803.html:text/html}
}

@article{frankle_early_2020,
	title = {The {Early} {Phase} of {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/2002.10365},
	abstract = {Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here, we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.},
	urldate = {2020-06-24},
	journal = {arXiv:2002.10365 [cs, stat]},
	author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.10365},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2020 Camera Ready. Available on OpenReview at https://openreview.net/forum?id=Hkl1iRNFwS},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\7W2UM6NF\\Frankle et al. - 2020 - The Early Phase of Neural Network Training.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\EIDIQKJ3\\2002.html:text/html}
}

@article{kang_accelerator-aware_2020,
	title = {Accelerator-{Aware} {Pruning} for {Convolutional} {Neural} {Networks}},
	issn = {1051-8215, 1558-2205},
	url = {http://arxiv.org/abs/1804.09862},
	doi = {10.1109/TCSVT.2019.2911674},
	abstract = {Convolutional neural networks have shown tremendous performance capabilities in computer vision tasks, but their excessive amounts of weight storage and arithmetic operations prevent them from being adopted in embedded environments. One of the solutions involves pruning, where certain unimportant weights are forced to have a value of zero. Many pruning schemes have been proposed, but these have mainly focused on the number of pruned weights. Previous pruning schemes scarcely considered ASIC or FPGA accelerator architectures. When these pruned networks are run on accelerators, the lack of consideration of the architecture causes some inefficiency problems, including internal buffer misalignments and load imbalances. This paper proposes a new pruning scheme that reflects accelerator architectures. In the proposed scheme, pruning is performed so that the same number of weights remain for each weight group corresponding to activations fetched simultaneously. In this way, the pruning scheme resolves the inefficiency problems, doubling the accelerator performance. Even with this constraint, the proposed pruning scheme reached a pruning ratio similar to that of previous unconstrained pruning schemes, not only on AlexNet and VGG16 but also on state-of-the-art very deep networks such as ResNet. Furthermore, the proposed scheme demonstrated a comparable pruning ratio on compact networks such as MobileNet and on slimmed networks that were already pruned in a channel-wise manner. In addition to improving the efficiency of previous sparse accelerators, it will be also shown that the proposed pruning scheme can be used to reduce the logic complexity of sparse accelerators.},
	urldate = {2020-06-24},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Kang, Hyeong-Ju},
	year = {2020},
	note = {arXiv: 1804.09862},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	pages = {1--1},
	annote = {Comment: 11 pages, 9 figures, accepted to IEEE Transactions on Circuits and Systems for Video Technology},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\A2KUCBVX\\Kang - 2020 - Accelerator-Aware Pruning for Convolutional Neural.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\RNITRZHT\\1804.html:text/html}
}

@article{han_learning_2015,
	title = {Learning both {Weights} and {Connections} for {Efficient} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1506.02626},
	abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.},
	urldate = {2020-06-24},
	journal = {arXiv:1506.02626 [cs]},
	author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
	month = oct,
	year = {2015},
	note = {arXiv: 1506.02626},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at NIPS 2015},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\MAS2D2GF\\Han et al. - 2015 - Learning both Weights and Connections for Efficien.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\3YTU68VE\\1506.html:text/html}
}

@article{mao_exploring_2017,
	title = {Exploring the {Regularity} of {Sparse} {Structure} in {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1705.08922},
	abstract = {Sparsity helps reduce the computational complexity of deep neural networks by skipping zeros. Taking advantage of sparsity is listed as a high priority in next generation DNN accelerators such as TPU. The structure of sparsity, i.e., the granularity of pruning, affects the efficiency of hardware accelerator design as well as the prediction accuracy. Coarse-grained pruning creates regular sparsity patterns, making it more amenable for hardware acceleration but more challenging to maintain the same accuracy. In this paper we quantitatively measure the trade-off between sparsity regularity and prediction accuracy, providing insights in how to maintain accuracy while having more a more structured sparsity pattern. Our experimental results show that coarse-grained pruning can achieve a sparsity ratio similar to unstructured pruning without loss of accuracy. Moreover, due to the index saving effect, coarse-grained pruning is able to obtain a better compression ratio than fine-grained sparsity at the same accuracy threshold. Based on the recent sparse convolutional neural network accelerator (SCNN), our experiments further demonstrate that coarse-grained sparsity saves about 2x the memory references compared to fine-grained sparsity. Since memory reference is more than two orders of magnitude more expensive than arithmetic operations, the regularity of sparse structure leads to more efficient hardware design.},
	urldate = {2020-06-24},
	journal = {arXiv:1705.08922 [cs, stat]},
	author = {Mao, Huizi and Han, Song and Pool, Jeff and Li, Wenshuo and Liu, Xingyu and Wang, Yu and Dally, William J.},
	month = jun,
	year = {2017},
	note = {arXiv: 1705.08922},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: submitted to NIPS 2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\IZC9SCJI\\Mao et al. - 2017 - Exploring the Regularity of Sparse Structure in Co.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\KKD2QWU8\\1705.html:text/html}
}

@article{tu_pruning_2019,
	title = {Pruning {Depthwise} {Separable} {Convolutions} for {Extra} {Efficiency} {Gain} of {Lightweight} {Models}},
	url = {https://openreview.net/forum?id=B1g5qyHYPS},
	abstract = {Deep convolutional neural networks are good at accuracy while bad at efficiency. To improve the inference speed, two kinds of directions are developed, lightweight model designing and network...},
	urldate = {2020-06-24},
	author = {Tu, Cheng-Hao and Lee, Jia-Hong and Chan, Yi-Ming and Chen, Chu-Song},
	month = sep,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\ZZS38XU4\\Tu et al. - 2019 - Pruning Depthwise Separable Convolutions for Extra.pdf:application/pdf;Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\GTHSRPKG\\forum.html:text/html}
}

@article{zhang_channel_2019,
	title = {A {Channel} {Pruning} {Algorithm} {Based} on {Depth}-{Wise} {Separable} {Convolution} {Unit}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2956976},
	abstract = {Deep learning has made significant progress in many fields such as image identification, speech recognition and natural language processing, especially in the field of computer vision. The better performance of the neural network often built on deeper, wider network structure, more network parameters and more storage and often computational expensive. As a result, it is hard to deploy neural network to mobile and embedded devices. Therefore, compressing of convolutional neural networks is very necessary and practical. In this paper, we propose a channel pruning algorithm for depth-wise separable convolution units and introduce a new channel selection algorithm based on information gain and a method for quickly recovering network performance after pruning. The proposed method is implemented on MobileNet and validated on several popular datasets. The experimental results show that our method can achieve better experimental results on several image classification datasets, and also achieve good detection results on the PASCAL VOC image detection dataset.},
	journal = {IEEE Access},
	author = {Zhang, Ke and Cheng, Ken and Li, Jingjing and Peng, Yuanyuan},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Convolution, deep learning, Deep learning, convolutional neural networks, Convolutional neural networks, Kernel, convolutional neural nets, computer vision, channel pruning, channel pruning algorithm, channel selection algorithm, Computational complexity, Computational efficiency, depth-wise separable convolution unit, embedded devices, image classification datasets, image identification, information gain, Matrix decomposition, mobile devices, MobileNet, natural language processing, network parameters, network performance, PASCAL VOC image detection dataset, speech recognition},
	pages = {173294--173309},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\CKNLQLIQ\\Zhang et al. - 2019 - A Channel Pruning Algorithm Based on Depth-Wise Se.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\4IXDLB6B\\8918317.html:text/html}
}

@inproceedings{baoyuan_liu_sparse_2015,
	address = {Boston, MA, USA},
	title = {Sparse {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298681/},
	doi = {10.1109/CVPR.2015.7298681},
	abstract = {Deep neural networks have achieved remarkable performance in both image classiﬁcation and object detection problems, at the cost of a large number of parameters and computational complexity. In this work, we show how to reduce the redundancy in these parameters using a sparse decomposition. Maximum sparsity is obtained by exploiting both inter-channel and intra-channel redundancy, with a ﬁne-tuning step that minimize the recognition loss caused by maximizing sparsity. This procedure zeros out more than 90\% of parameters, with a drop of accuracy that is less than 1\% on the ILSVRC2012 dataset. We also propose an efﬁcient sparse matrix multiplication algorithm on CPU for Sparse Convolutional Neural Networks (SCNN) models. Our CPU implementation demonstrates much higher efﬁciency than the off-the-shelf sparse matrix libraries, with a signiﬁcant speedup realized over the original dense network. In addition, we apply the SCNN model to the object detection problem, in conjunction with a cascade model and sparse fully connected layers, to achieve signiﬁcant speedups.},
	language = {en},
	urldate = {2020-06-26},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {{Baoyuan Liu} and {Min Wang} and Foroosh, Hassan and Tappen, Marshall and Penksy, Marianna},
	month = jun,
	year = {2015},
	pages = {806--814},
	file = {Baoyuan Liu et al. - 2015 - Sparse Convolutional Neural Networks.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\NLAU5XHG\\Baoyuan Liu et al. - 2015 - Sparse Convolutional Neural Networks.pdf:application/pdf}
}

@inproceedings{shimoda_filter-wise_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Filter-{Wise} {Pruning} {Approach} to {FPGA} {Implementation} of {Fully} {Convolutional} {Network} for {Semantic} {Segmentation}},
	isbn = {978-3-030-17227-5},
	doi = {10.1007/978-3-030-17227-5_26},
	abstract = {This paper presents a hardware-aware sparse fully convolutional network (SFCN) for semantic segmentation on an FPGA. Semantic segmentation attracts interest since for self-driving car it is important to recognize road and obstacles in pixel level. However, it is hard to implement the system on embedded systems since the number of weights for the SFCN is so large that embedded systems cannot store them using limited on-chip memory. To realize good a trade-off between speed and accuracy, we construct an AlexNet-based SFCN which has no skip connections and deconvolution layers to reduce the computation costs and the latency. Furthermore, we propose a filter-wise pruning technique that sorts the weights of each filter by their absolute values and prunes them by a preset percent filter-by-filter from a small order. It is more suitable for the hardware implementation since the number of computation of each filter becomes equal. We trained the AlexNet-based SFCN by using Camvid image dataset and implemented on Xilinx zcu102 evaluation board. The results show that the FPGA system is 10.14 times faster than a mobile GPU one, and its performance per power consumption is 24.49 times higher than the GPU counterpart.},
	language = {en},
	booktitle = {Applied {Reconfigurable} {Computing}},
	publisher = {Springer International Publishing},
	author = {Shimoda, Masayuki and Sada, Youki and Nakahara, Hiroki},
	editor = {Hochberger, Christian and Nelson, Brent and Koch, Andreas and Woods, Roger and Diniz, Pedro},
	year = {2019},
	keywords = {FPGA, Fully convolutional network, Semantic segmentation, Sparse neural network},
	pages = {371--386}
}

@article{wen_learning_2016,
	title = {Learning {Structured} {Sparsity} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.03665},
	abstract = {High demand for computation resources severely hinders deployment of large-scale Deep Neural Networks (DNN) in resource constrained devices. In this work, we propose a Structured Sparsity Learning (SSL) method to regularize the structures (i.e., filters, channels, filter shapes, and layer depth) of DNNs. SSL can: (1) learn a compact structure from a bigger DNN to reduce computation cost; (2) obtain a hardware-friendly structured sparsity of DNN to efficiently accelerate the DNNs evaluation. Experimental results show that SSL achieves on average 5.1x and 3.1x speedups of convolutional layer computation of AlexNet against CPU and GPU, respectively, with off-the-shelf libraries. These speedups are about twice speedups of non-structured sparsity; (3) regularize the DNN structure to improve classification accuracy. The results show that for CIFAR-10, regularization on layer depth can reduce 20 layers of a Deep Residual Network (ResNet) to 18 layers while improve the accuracy from 91.25\% to 92.60\%, which is still slightly higher than that of original ResNet with 32 layers. For AlexNet, structure regularization by SSL also reduces the error by around {\textasciitilde}1\%. Open source code is in https://github.com/wenwei202/caffe/tree/scnn},
	urldate = {2020-06-26},
	journal = {arXiv:1608.03665 [cs, stat]},
	author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
	month = oct,
	year = {2016},
	note = {arXiv: 1608.03665},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning, I.5.1},
	annote = {Comment: Accepted by NIPS 2016},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\M8IVP3ZQ\\Wen et al. - 2016 - Learning Structured Sparsity in Deep Neural Networ.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\MBSACST9\\1608.html:text/html}
}

@book{harris_digital_2015,
	address = {San Francisco, CA, USA},
	edition = {1st},
	title = {Digital {Design} and {Computer} {Architecture}: {ARM} {Edition}},
	isbn = {978-0-12-800056-4},
	shorttitle = {Digital {Design} and {Computer} {Architecture}},
	abstract = {Digital Design and Computer Architecture: ARM Edition takes a unique and modern approach to digital design. Beginning with digital logic gates and progressing to the design of combinational and sequential circuits, Harris and Harris use these fundamental building blocks as the basis for what follows: the design of an actual ARM processor. With over 75\% of the worlds population using products with ARM processors, the design of the ARM processor offers an exciting and timely application of digital design while also teaching the fundamentals of computer architecture. System Verilog and VHDL are integrated throughout the text in examples illustrating the methods and techniques for CAD-based circuit design. By the end of this book, readers will be able to build their own microprocessor and will have a top-to-bottom understanding of how it works. Harris and Harris have combined an engaging and humorous writing style with an updated and hands-on approach to digital design. Covers the fundamentals of digital logic design and reinforces logic concepts through the design of an ARM microprocessor. Features side-by-side examples of the two most prominent Hardware Description Languages (HDLs)-System Verilog and VHDL-which illustrate and compare the ways each can be used in the design of digital systems. Includes examples throughout the text that enhance the readers understanding and retention of key concepts and techniques. The Companion website includes a chapter on I/O systems with practical examples that show how to use the Raspberry Pi computer to communicate with peripheral devices such as LCDs, Bluetooth radios, and motors. The Companion website also includes appendices covering practical digital design issues and C programming as well as links to CAD tools, lecture slides, laboratory projects, and solutions to exercises.},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Harris, Sarah and Harris, David},
	year = {2015},
	file = {Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\DVRWRMPQ\\Harris et Harris - 2015 - Digital Design and Computer Architecture ARM Edit.pdf:application/pdf}
}

@article{chollet_xception_2017,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://arxiv.org/abs/1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	urldate = {2020-06-27},
	journal = {arXiv:1610.02357 [cs]},
	author = {Chollet, François},
	month = apr,
	year = {2017},
	note = {arXiv: 1610.02357},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\53AMMXHC\\Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\DK3CAI88\\1610.html:text/html}
}

@book{sifre_ecole_2014,
	title = {Ecole {Polytechnique}, {CMAP} {PhD} thesis {Rigid}-{Motion} {Scattering} {For} {Image} {Classification} {Author}:},
	shorttitle = {Ecole {Polytechnique}, {CMAP} {PhD} thesis {Rigid}-{Motion} {Scattering} {For} {Image} {Classification} {Author}},
	abstract = {Image classification is the problem of assigning a label that best describes the content of unknown images, given a set of training images with known labels. This thesis introduces image classification algorithms based on the scattering transform, studies their properties and describes extensive classification experiments on challenging texture and object image datasets. Images are high dimensional signals for which generic machine learning algorithms fail when applied directly on the raw pixel space. Therefore, most successful approaches involve building a specific low dimensional representation on which the classification is performed. Traditionally, the representation was engineered to reduce the dimensionality of images by building invariance to geometric transformations while retaining discriminative features. More recently, deep convolutional networks have achieved state-of-the-art results on most image classification tasks. Such networks progressively build more invariant representations through a hierarchy of convolutional layers where all the weights are learned. This thesis proposes several scattering representations. Those scattering representa-tions have a structure similar to convolutional networks, but the weights of scattering are},
	author = {Sifre, Laurent and Mallat, Prof Stéphane},
	year = {2014},
	file = {Citeseer - Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\DVFHZ65N\\summary.html:text/html;Citeseer - Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\9YCWSHD8\\Sifre et Mallat - 2014 - Ecole Polytechnique, CMAP PhD thesis Rigid-Motion .pdf:application/pdf}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2020-06-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105},
	file = {NIPS Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\GDK7JZXF\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\6XX7JJW3\\4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html}
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	url = {http://www.deeplearningbook.org},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016}
}

@inproceedings{suda_throughput-optimized_2016,
	address = {Monterey, California, USA},
	title = {Throughput-{Optimized} {OpenCL}-based {FPGA} {Accelerator} for {Large}-{Scale} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4503-3856-1},
	url = {http://dl.acm.org/citation.cfm?doid=2847263.2847276},
	doi = {10.1145/2847263.2847276},
	abstract = {Convolutional Neural Networks (CNNs) have gained popularity in many computer vision applications such as image classification, face detection, and video analysis, because of their ability to train and classify with high accuracy. Due to multiple convolution and fully-connected layers that are compute/memory-intensive, it is difficult to perform real-time classification with low power consumption on today’s computing systems. FPGAs have been widely explored as hardware accelerators for CNNs because of their reconfigurability and energy efficiency, as well as fast turn-around-time, especially with high-level synthesis methodologies. Previous FPGA-based CNN accelerators, however, typically implemented generic accelerators agnostic to the CNN configuration, where the reconfigurable capabilities of FPGAs are not fully leveraged to maximize the overall system throughput. In this work, we present a systematic design space exploration methodology to maximize the throughput of an OpenCL-based FPGA accelerator for a given CNN model, considering the FPGA resource constraints such as on-chip memory, registers, computational resources and external memory bandwidth. The proposed methodology is demonstrated by optimizing two representative large-scale CNNs, AlexNet and VGG, on two Altera Stratix-V FPGA platforms, DE5-Net and P395-D8 boards, which have different hardware resources. We achieve a peak performance of 136.5 GOPS for convolution operation, and 117.8 GOPS for the entire VGG network that performs ImageNet classification on P395-D8 board.},
	language = {en},
	urldate = {2020-06-28},
	booktitle = {Proceedings of the 2016 {ACM}/{SIGDA} {International} {Symposium} on {Field}-{Programmable} {Gate} {Arrays} - {FPGA} '16},
	publisher = {ACM Press},
	author = {Suda, Naveen and Chandra, Vikas and Dasika, Ganesh and Mohanty, Abinash and Ma, Yufei and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu},
	year = {2016},
	pages = {16--25},
	file = {Suda et al. - 2016 - Throughput-Optimized OpenCL-based FPGA Accelerator.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\PDWIELEQ\\Suda et al. - 2016 - Throughput-Optimized OpenCL-based FPGA Accelerator.pdf:application/pdf}
}

@article{glorot_understanding_2010,
	title = {Understanding the difﬁculty of training deep feedforward neural networks},
	abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	language = {en},
	author = {Glorot, Xavier and Bengio, Yoshua},
	year = {2010},
	pages = {8},
	file = {Glorot et Bengio - Understanding the difﬁculty of training deep feedf.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\M37FVI3N\\Glorot et Bengio - Understanding the difﬁculty of training deep feedf.pdf:application/pdf}
}

@article{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	shorttitle = {Delving {Deep} into {Rectifiers}},
	url = {http://arxiv.org/abs/1502.01852},
	abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
	urldate = {2020-06-29},
	journal = {arXiv:1502.01852 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.01852},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\RF2EYDCE\\He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\IDBEXI9N\\1502.html:text/html}
}

@article{ruder_overview_2017,
	title = {An overview of gradient descent optimization algorithms},
	url = {http://arxiv.org/abs/1609.04747},
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	urldate = {2020-06-29},
	journal = {arXiv:1609.04747 [cs]},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv: 1609.04747},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Added derivations of AdaMax and Nadam},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\VDTHKM3X\\Ruder - 2017 - An overview of gradient descent optimization algor.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\8RT2BA8N\\1609.html:text/html}
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2020-06-30},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\QLMK3JTU\\Simonyan et Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\YPNXIBWQ\\1409.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2020-06-30},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\45D4WKLJ\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\DBGY74XY\\1512.html:text/html}
}

@article{maas_rectier_2014,
	title = {Rectiﬁer {Nonlinearities} {Improve} {Neural} {Network} {Acoustic} {Models}},
	abstract = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectiﬁed linear (ReL) hidden units demonstrates additional gains in ﬁnal system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectiﬁer networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectiﬁer nonlinearities produce 2\% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify diﬀerences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectiﬁer networks.},
	language = {en},
	author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y},
	year = {2014},
	pages = {6},
	file = {Maas et al. - Rectiﬁer Nonlinearities Improve Neural Network Aco.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\NGF4G4HM\\Maas et al. - Rectiﬁer Nonlinearities Improve Neural Network Aco.pdf:application/pdf}
}

@article{david_hardware_2007,
	title = {Hardware {Complexity} of {Modular} {Multiplication} and {Exponentiation}},
	volume = {56},
	issn = {1557-9956},
	doi = {10.1109/TC.2007.1084},
	abstract = {Large integer modular multiplication (MM) and modular exponentiation (ME) are the foundation of most public-key cryptosystems, specifically RSA, Diffie-Helleman, EIGamal, and the elliptic curve cryptosystems. Thus, MM algorithms have been studied widely and extensively. Most of the work is based on the well-known Montgomery multiplication method and its variants, which require standard multiplication operations. Despite their better complexity orders, Karatsuba and FFT algorithms seem to rarely be used for hardware implementation. In this paper, we review their hardware complexity and propose original implementations of MM and ME that become useful for 24-bit operators (Karatsuba algorithm) or 373-bit operators (FFT algorithm).},
	number = {10},
	journal = {IEEE Transactions on Computers},
	author = {David, Jean Pierre and Kalach, Kassem and Tittley, Nicolas},
	month = oct,
	year = {2007},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {computational complexity, Complexity theory, Adders, Algorithm design and analysis, Atomic measurements, Cryptography, Elliptic curve cryptography, hardware complexity, Hardware Complexity, integer modular multiplication, Logic gates, Modular Arithmetic, modular exponentiation, Montgomery multiplication, Multiplication, public key cryptography, public-key cryptosystem, Security},
	pages = {1308--1319},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\UU5X7H4F\\4302704.html:text/html}
}

@article{canziani_analysis_2017,
	title = {An {Analysis} of {Deep} {Neural} {Network} {Models} for {Practical} {Applications}},
	url = {http://arxiv.org/abs/1605.07678},
	abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the ﬁeld of computer vision, the ImageNet classiﬁcation challenge has played a major role in advancing the state-of-the-art. While accuracy ﬁgures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key ﬁndings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efﬁcient DNNs.},
	language = {en},
	urldate = {2020-07-08},
	journal = {arXiv:1605.07678 [cs]},
	author = {Canziani, Alfredo and Paszke, Adam and Culurciello, Eugenio},
	month = apr,
	year = {2017},
	note = {arXiv: 1605.07678},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 7 pages, 10 figures, legend for Figure 2 got lost :/},
	file = {Canziani et al. - 2017 - An Analysis of Deep Neural Network Models for Prac.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\RTJB7K2K\\Canziani et al. - 2017 - An Analysis of Deep Neural Network Models for Prac.pdf:application/pdf}
}

@article{denton_exploiting_2014,
	title = {Exploiting {Linear} {Structure} {Within} {Convolutional} {Networks} for {Efficient} {Evaluation}},
	url = {http://arxiv.org/abs/1404.0736},
	abstract = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of ﬂoating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional ﬁlters to derive approximations that signiﬁcantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2×, while keeping the accuracy within 1\% of the original model.},
	language = {en},
	urldate = {2020-07-07},
	journal = {arXiv:1404.0736 [cs]},
	author = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
	month = jun,
	year = {2014},
	note = {arXiv: 1404.0736},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Denton et al. - 2014 - Exploiting Linear Structure Within Convolutional N.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\4332K8DR\\Denton et al. - 2014 - Exploiting Linear Structure Within Convolutional N.pdf:application/pdf}
}

@article{zhao_towards_2018,
	title = {Towards {Efficient} {Convolutional} {Neural} {Network} for {Domain}-{Specific} {Applications} on {FPGA}},
	url = {http://arxiv.org/abs/1809.03318},
	abstract = {FPGA becomes a popular technology for implementing Convolutional Neural Network (CNN) in recent years. Most CNN applications on FPGA are domain-speciﬁc, e.g., detecting objects from speciﬁc categories, in which commonlyused CNN models pre-trained on general datasets may not be efﬁcient enough. This paper presents TuRF, an end-to-end CNN acceleration framework to efﬁciently deploy domain-speciﬁc applications on FPGA by transfer learning that adapts pre-trained models to speciﬁc domains, replacing standard convolution layers with efﬁcient convolution blocks, and applying layer fusion to enhance hardware design performance. We evaluate TuRF by deploying a pre-trained VGG-16 model for a domain-speciﬁc image recognition task onto a Stratix V FPGA. Results show that designs generated by TuRF achieve better performance than prior methods for the original VGG-16 and ResNet-50 models, while for the optimised VGG-16 model TuRF designs are more accurate and easier to process.},
	language = {en},
	urldate = {2020-07-06},
	journal = {arXiv:1809.03318 [cs]},
	author = {Zhao, Ruizhe and Ng, Ho-Cheung and Luk, Wayne and Niu, Xinyu},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.03318},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Zhao et al. - 2018 - Towards Efficient Convolutional Neural Network for.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\7HSKGYPJ\\Zhao et al. - 2018 - Towards Efficient Convolutional Neural Network for.pdf:application/pdf}
}

@inproceedings{zhang_caffeine_2016,
	address = {Austin Texas},
	title = {Caffeine: towards uniformed representation and acceleration for deep convolutional neural networks},
	isbn = {978-1-4503-4466-1},
	shorttitle = {Caffeine},
	url = {https://dl.acm.org/doi/10.1145/2966986.2967011},
	doi = {10.1145/2966986.2967011},
	abstract = {With the recent advancement of multilayer convolutional neural networks (CNN) and fully connected networks (FCN), deep learning has achieved amazing success in many areas, especially in visual content understanding and classiﬁcation. To improve the performance and energy efﬁciency of the computation-demanding CNN, the FPGAbased acceleration emerges as one of the most attractive alternatives.},
	language = {en},
	urldate = {2020-07-09},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Computer}-{Aided} {Design}},
	publisher = {ACM},
	author = {Zhang, Chen and Fang, Zhenman and Zhou, Peipei and Pan, Peichen and Cong, Jason},
	month = nov,
	year = {2016},
	pages = {1--8},
	file = {Zhang et al. - 2016 - Caffeine towards uniformed representation and acc.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\BQ4TUUZG\\Zhang et al. - 2016 - Caffeine towards uniformed representation and acc.pdf:application/pdf}
}

@article{anwar_structured_2017,
	title = {Structured {Pruning} of {Deep} {Convolutional} {Neural} {Networks}},
	volume = {13},
	issn = {1550-4832, 1550-4840},
	url = {https://dl.acm.org/doi/10.1145/3005348},
	doi = {10.1145/3005348},
	abstract = {Real time application of deep learning algorithms is often hindered by high computational complexity and frequent memory accesses. Network pruning is a promising technique to solve this problem. However, pruning usually results in irregular network connections that not only demand extra representation efforts but also do not fit well on parallel computation. We introduce structured sparsity at various scales for convolutional neural networks, which are channel wise, kernel wise and intra kernel strided sparsity. This structured sparsity is very advantageous for direct computational resource savings on embedded computers, parallel computing environments and hardware based systems. To decide the importance of network connections and paths, the proposed method uses a particle filtering approach. The importance weight of each particle is assigned by computing the misclassification rate with corresponding connectivity pattern. The pruned network is re-trained to compensate for the losses due to pruning. While implementing convolutions as matrix products, we particularly show that intra kernel strided sparsity with a simple constraint can significantly reduce the size of kernel and feature map tensors. The pruned network is finally quantized with reduced word length precision. This results in significant reduction in the total storage size providing advantages for on-chip memory based implementations of deep neural networks.},
	language = {en},
	number = {3},
	urldate = {2020-07-11},
	journal = {ACM Journal on Emerging Technologies in Computing Systems},
	author = {Anwar, Sajid and Hwang, Kyuyeon and Sung, Wonyong},
	month = may,
	year = {2017},
	pages = {1--18},
	file = {Anwar et al. - 2017 - Structured Pruning of Deep Convolutional Neural Ne.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\KQNQ3D8W\\Anwar et al. - 2017 - Structured Pruning of Deep Convolutional Neural Ne.pdf:application/pdf}
}

@article{mittal_survey_2014,
	title = {A {Survey} of {Methods} {For} {Analyzing} and {Improving} {GPU} {Energy} {Efficiency}},
	url = {http://arxiv.org/abs/1404.4629},
	abstract = {Recent years have witnessed a phenomenal growth in the computational capabilities and applications of GPUs. However, this trend has also led to dramatic increase in their power consumption. This paper surveys research works on analyzing and improving energy efficiency of GPUs. It also provides a classification of these techniques on the basis of their main research idea. Further, it attempts to synthesize research works which compare energy efficiency of GPUs with other computing systems, e.g. FPGAs and CPUs. The aim of this survey is to provide researchers with knowledge of state-of-the-art in GPU power management and motivate them to architect highly energy-efficient GPUs of tomorrow.},
	urldate = {2020-07-11},
	journal = {arXiv:1404.4629 [cs]},
	author = {Mittal, Sparsh and Vetter, Jeffrey S.},
	month = apr,
	year = {2014},
	note = {arXiv: 1404.4629},
	keywords = {Computer Science - Hardware Architecture, A.1, H.3.4, I.3.1},
	annote = {Comment: Accepted with minor revision in ACM Computing Survey Journal (impact factor 3.85, five year impact of 7.85)},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\BR4HH65J\\Mittal et Vetter - 2014 - A Survey of Methods For Analyzing and Improving GP.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\82N6JH7P\\1404.html:text/html}
}

@article{vestias_fast_2019,
	title = {Fast {Convolutional} {Neural} {Networks} in {Low} {Density} {FPGAs} {Using} {Zero}-{Skipping} and {Weight} {Pruning}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2079-9292/8/11/1321},
	doi = {10.3390/electronics8111321},
	abstract = {Edge devices are becoming smarter with the integration of machine learning methods, such as deep learning, and are therefore used in many application domains where decisions have to be made without human intervention. Deep learning and, in particular, convolutional neural networks (CNN) are more efficient than previous algorithms for several computer vision applications such as security and surveillance, where image and video analysis are required. This better efficiency comes with a cost of high computation and memory requirements. Hence, running CNNs in embedded computing devices is a challenge for both algorithm and hardware designers. New processing devices, dedicated system architectures and optimization of the networks have been researched to deal with these computation requirements. In this paper, we improve the inference execution times of CNNs in low density FPGAs (Field-Programmable Gate Arrays) using fixed-point arithmetic, zero-skipping and weight pruning. The developed architecture supports the execution of large CNNs in FPGA devices with reduced on-chip memory and computing resources. With the proposed architecture, it is possible to infer an image in AlexNet in 2.9 ms in a ZYNQ7020 and 1.0 ms in a ZYNQ7045 with less than 1\% accuracy degradation. These results improve previous state-of-the-art architectures for CNN inference.},
	language = {en},
	number = {11},
	urldate = {2020-07-11},
	journal = {Electronics},
	author = {Véstias, Mário P. and Duarte, Rui Policarpo and de Sousa, José T. and Neto, Horácio C.},
	month = nov,
	year = {2019},
	note = {Number: 11
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {convolutional neural network, FPGA, deep learning, pruning, smart edge devices, zero-skipping},
	pages = {1321},
	file = {Full Text PDF:C\:\\Users\\Guigui\\Zotero\\storage\\49J3ENVN\\Véstias et al. - 2019 - Fast Convolutional Neural Networks in Low Density .pdf:application/pdf;Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\ERLBH4FG\\htm.html:text/html}
}

@inproceedings{horowitz_11_2014,
	title = {1.1 {Computing}'s energy problem (and what we can do about it)},
	doi = {10.1109/ISSCC.2014.6757323},
	abstract = {Our challenge is clear: The drive for performance and the end of voltage scaling have made power, and not the number of transistors, the principal factor limiting further improvements in computing performance. Continuing to scale compute performance will require the creation and effective use of new specialized compute engines, and will require the participation of application experts to be successful. If we play our cards right, and develop the tools that allow our customers to become part of the design process, we will create a new wave of innovative and efficient computing devices.},
	booktitle = {2014 {IEEE} {International} {Solid}-{State} {Circuits} {Conference} {Digest} of {Technical} {Papers} ({ISSCC})},
	author = {Horowitz, Mark},
	month = feb,
	year = {2014},
	note = {ISSN: 2376-8606},
	keywords = {Hardware, Energy efficiency, Logic gates, CMOS integrated circuits, CMOS technology, compute engines, computing performance, innovative computing devices, performance evaluation, power, power aware computing, search engines, transistors, Transistors, Voltage control, voltage scaling},
	pages = {10--14},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\GGCYFTQH\\6757323.html:text/html}
}

@article{kim_zena_2018,
	title = {{ZeNA}: {Zero}-{Aware} {Neural} {Network} {Accelerator}},
	volume = {35},
	issn = {2168-2364},
	shorttitle = {{ZeNA}},
	doi = {10.1109/MDAT.2017.2741463},
	abstract = {It has been observed that the majority of the kernel weights and input activations in the state-of-the-art convolution neural networks (CNNs) have zero values. This article proposes a CNN hardware accelerator that exploits this property to achieve significant performance and energy improvements.},
	number = {1},
	journal = {IEEE Design Test},
	author = {Kim, Dongyoung and Ahn, Junwhan and Yoo, Sungjoo},
	month = feb,
	year = {2018},
	note = {Conference Name: IEEE Design Test},
	keywords = {accelerator architecture, Computer architecture, neural nets, deep learning, Neural networks, Machine learning, Convolutional neural networks, CNN hardware accelerator, convolution neural networks, Convolutional neural networks (CNNs), Random access memory, Resource management, ZeNA, zero value, zero-aware neural network accelerator},
	pages = {39--46},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Guigui\\Zotero\\storage\\GQQRSGAV\\8013151.html:text/html}
}

@unpublished{matteucci_artificial_2019,
	address = {Politecnico di Milano},
	type = {Lecture notes},
	title = {Artificial {Neural} {Networks} and {Deep} {Learning}},
	language = {English},
	urldate = {2020-07-10},
	author = {Matteucci, Matteo and Boracchi, Giacomo},
	year = {2019}
}

@article{krizhevsky_convolutional_nodate,
	title = {Convolutional {Deep} {Belief} {Networks} on {CIFAR}-10},
	language = {en},
	author = {Krizhevsky, Alex},
	pages = {9},
	file = {Krizhevsky - Convolutional Deep Belief Networks on CIFAR-10.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\TILL8IWM\\Krizhevsky - Convolutional Deep Belief Networks on CIFAR-10.pdf:application/pdf}
}

@book{minsky_perceptrons_1972,
	title = {Perceptrons: {An} {Introduction} to {Computational} {Geometry}},
	isbn = {978-0-262-63022-1},
	shorttitle = {Perceptrons},
	language = {en},
	publisher = {Mit Press},
	author = {Minsky, Marvin Lee and Papert, Seymour},
	year = {1972},
	note = {Google-Books-ID: Ow1OAQAAIAAJ},
	keywords = {Computers / Artificial Intelligence, Mathematics / Geometry / General, Perceptrons}
}

@unpublished{ward_real-time_2001,
	address = {Imperial College of science, technology and medicine, University of London},
	type = {Lecture notes},
	title = {Real-time {Digital} {Signal} {Processing} with the {TMS320C6000}},
	url = {http://www.ee.ic.ac.uk/pcheung/teaching/ee3_Study_Project/},
	language = {English},
	urldate = {2020-07-26},
	author = {Ward, Darren and Brookes, Mike and Cheung, Peter Y. K.},
	year = {2001}
}

@inproceedings{buluc_parallel_2009,
	address = {Calgary, AB, Canada},
	title = {Parallel sparse matrix-vector and matrix-transpose-vector multiplication using compressed sparse blocks},
	isbn = {978-1-60558-606-9},
	url = {http://portal.acm.org/citation.cfm?doid=1583991.1584053},
	doi = {10.1145/1583991.1584053},
	abstract = {This paper introduces a storage format for sparse matrices, called compressed sparse blocks (CSB), which allows both Ax and ATx to be computed efﬁciently in parallel, where A is an n × n sparse r(mictrahittmriicxsaulw-spietahtΘhn(nnleznn≥zg)tnwh)on, rokyniz(eseledroriinsaglanraudnpxnaiirnsaglaltedilmeisnems)eaonnf-dvΘeΘc((tn√onr.nz /lOg√unrn)alsglpgnao)n-, which is amply high for virtually any large matrix. The storage requirement for CSB is esssentially the same as that for the morestandard compressed-sparse-rows (CSR) format, for which computing Ax in parallel is easy but ATx is difﬁcult. Benchmark results indicate that on one processor, the CSB algorithms for Ax and ATx run just as fast as the CSR algorithm for Ax, but the CSB algorithms also scale up linearly with processors until limited by offchip memory bandwidth.},
	language = {en},
	urldate = {2020-08-05},
	booktitle = {Proceedings of the twenty-first annual symposium on {Parallelism} in algorithms and architectures - {SPAA} '09},
	publisher = {ACM Press},
	author = {Buluç, Aydin and Fineman, Jeremy T. and Frigo, Matteo and Gilbert, John R. and Leiserson, Charles E.},
	year = {2009},
	pages = {233},
	file = {Buluç et al. - 2009 - Parallel sparse matrix-vector and matrix-transpose.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\96YB3WPZ\\Buluç et al. - 2009 - Parallel sparse matrix-vector and matrix-transpose.pdf:application/pdf}
}

@misc{technologies_terasic_nodate,
	title = {Terasic - {SoC} {Platform} - {Cyclone} - {DE10}-{Nano} {Kit}},
	url = {https://www.terasic.com.tw/cgi-bin/page/archive.pl?Language=English&No=1046},
	abstract = {Cyclone V SoC with Dual-core ARM Cortex-A9 (HPS) 1GB DDR3 SDRAM (32-bit data bus)(HPS) Arduino Expansion Header (Uno R3 Compatibility), Full HD HDMI Output, UART-to-USB, USB OTG Port, Micro SD Card Socket, Gigabit Ethernet and GPIO Headers},
	urldate = {2020-08-11},
	author = {Technologies, Terasic},
	file = {Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\MTYRN6JE\\archive.html:text/html}
}

@article{noauthor_cyclone_nodate,
	title = {Cyclone {V} {Device} {Overview}},
	language = {en},
	pages = {37},
	file = {Cyclone V Device Overview.pdf:C\:\\Users\\Guigui\\Zotero\\storage\\C69H4H4E\\Cyclone V Device Overview.pdf:application/pdf}
}

@article{jacob_quantization_2017,
	title = {Quantization and {Training} of {Neural} {Networks} for {Efficient} {Integer}-{Arithmetic}-{Only} {Inference}},
	url = {http://arxiv.org/abs/1712.05877},
	abstract = {The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.},
	urldate = {2020-08-12},
	journal = {arXiv:1712.05877 [cs, stat]},
	author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.05877},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 14 pages, 12 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\29RCT6Y5\\Jacob et al. - 2017 - Quantization and Training of Neural Networks for E.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\HQFQ439F\\1712.html:text/html}
}

@article{wu_quantized_2016,
	title = {Quantized {Convolutional} {Neural} {Networks} for {Mobile} {Devices}},
	url = {http://arxiv.org/abs/1512.06473},
	abstract = {Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response. Extensive experiments on the ILSVRC-12 benchmark demonstrate 4{\textasciitilde}6x speed-up and 15{\textasciitilde}20x compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second.},
	urldate = {2020-08-12},
	journal = {arXiv:1512.06473 [cs]},
	author = {Wu, Jiaxiang and Leng, Cong and Wang, Yuhang and Hu, Qinghao and Cheng, Jian},
	month = may,
	year = {2016},
	note = {arXiv: 1512.06473},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2016},
	file = {arXiv Fulltext PDF:C\:\\Users\\Guigui\\Zotero\\storage\\TUITK72A\\Wu et al. - 2016 - Quantized Convolutional Neural Networks for Mobile.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Guigui\\Zotero\\storage\\5URFJI2J\\1512.html:text/html}
}

@misc{noauthor_imagenet_nodate,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Competition} 2012 ({ILSVRC2012})},
	url = {http://image-net.org/challenges/LSVRC/2012/results.html#t1},
	language = {English},
	urldate = {2020-08-12},
	journal = {Image-net},
	file = {ImageNet Large Scale Visual Recognition Competition 2012 (ILSVRC2012):C\:\\Users\\Guigui\\Zotero\\storage\\6AK9EWFP\\results.html:text/html}
}

@article{morcel_feathernet_2019,
	title = {{FeatherNet}: {An} {Accelerated} {Convolutional} {Neural} {Network} {Design} for {Resource}-constrained {FPGAs}},
	volume = {12},
	issn = {1936-7406},
	shorttitle = {{FeatherNet}},
	url = {https://doi.org/10.1145/3306202},
	doi = {10.1145/3306202},
	abstract = {Convolutional Neural Network (ConvNet or CNN) algorithms are characterized by a large number of model parameters and high computational complexity. These two requirements have made it challenging for implementations on resource-limited FPGAs. The challenges are magnified when considering designs for low-end FPGAs. While previous work has demonstrated successful ConvNet implementations with high-end FPGAs, this article presents a ConvNet accelerator design that enables the implementation of complex deep ConvNet architectures on resource-constrained FPGA platforms aimed at the IoT market. We call the design “FeatherNet” for its light resource utilization. The implementations are VHDL-based providing flexibility in design optimizations. As part of the design process, new methods are introduced to address several design challenges. The first method is a novel stride-aware graph-based method targeted at ConvNets that aims at achieving efficient signal processing with reduced resource utilization. The second method addresses the challenge of determining the minimal precision arithmetic needed while preserving high accuracy. For this challenge, we propose variable-width dynamic fixed-point representations combined with a layer-by-layer design-space pruning heuristic across the different layers of the deep ConvNet model. The third method aims at achieving a modular design that can support different types of ConvNet layers while ensuring low resource utilization. For this challenge, we propose the modules to be relatively small and composed of computational filters that can be interconnected to build an entire accelerator design. These model elements can be easily configured through HDL parameters (e.g., layer type, mask size, stride, etc.) to meet the needs of specific ConvNet implementations and thus they can be reused to implement a wide variety of ConvNet architectures. The fourth method addresses the challenge of design portability between two different FPGA vendor platforms, namely, Intel/Altera and Xilinx. For this challenge, we propose to instantiate the device-specific hardware blocks needed in each computational filter, rather than relying on the synthesis tools to infer these blocks, while keeping track of the similarities and differences between the two platforms. We believe that the solutions to these design challenges further advance knowledge as they can benefit designers and other researchers using similar devices or facing similar challenges. Our results demonstrated the success of addressing the design challenges and achieving low (30\%) resource utilization for the low-end FPGA platforms: Zedboard and Cyclone V. The design overcame the limitation of designs targeted for high-end platforms and that cannot fit on low-end IoT platforms. Furthermore, our design showed superior performance results (measured in terms of [Frame/s/W] per Dollar) compared to high-end optimized designs.},
	number = {2},
	urldate = {2020-08-12},
	journal = {ACM Transactions on Reconfigurable Technology and Systems},
	author = {Morcel, Raghid and Hajj, Hazem and Saghir, Mazen A. R. and Akkary, Haitham and Artail, Hassan and Khanna, Rahul and Keshavamurthy, Anil},
	month = mar,
	year = {2019},
	keywords = {Convolutional neural networks, embedded-vision, IoT applications, resource-constrained FPGAs},
	pages = {6:1--6:27}
}
