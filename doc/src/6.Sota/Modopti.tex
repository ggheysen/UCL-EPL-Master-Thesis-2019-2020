\chapter{Accelerating \acrshort{cnn} inference on \acrshort{fpga}}
\label{chap:inf}
As said at chapter \ref{chap:intr}, interest has been made on accelerating the inference phase on \acrshort{fpga}. This chapter reviews the acceleration approaches to perform an efficient inference. \newline \newline
The three main optimization can be categorized according to \cite{abdelouahab_accelerating_2018}:
\begin{itemize}
    \item \textbf{Algorithmic Optimizations}: the computational cost of the convolution can be reduced by vectorizing them. More details can be found in section \ref{sec:algopti}.
    \item \textbf{Datapath Optimization}: because of the limited resources on a \acrshort{fpga}, memory if often the bottleneck and optimizing the memory management can increase the throughput. More details can be found in section \ref{sec:dtptopti}.
    \item \textbf{\acrshort{cnn} model Optimization}: an important issue of \acrshort{cnn} is their computational complexity and their hardware utilization. A solution is then to use approximate computing and trade accury for acceleration. This work focus on this kind of optimization. More details can be found in section \ref{sec:mdopti}.
\end{itemize}
The following sections introduce and discuss the different optimizations. Then we compare the most relevant approaches on \acrshort{fpga} on section \ref{sec:cclopti}.
%%%%%%%%%%%%
\input{src/6.Sota/sections/opti_algo}
%%%%%%%%%%%%
\input{src/6.Sota/sections/opti_dtpt}
%%%%%%%%%%%%
\input{src/6.Sota/sections/opti_md}
%%%%%%%%%%%%
\input{src/6.Sota/sections/opti_ccl}
