\section{Model Optimizations} \label{sec:mdopti}
According to \cite{chen_eyeriss_2017}, it is important to reduce the size of a model because the data-movement can often be more energy-consuming than actual computation. In this section we will review different approaches to reduce the size and the computational complexity of a model which can be an issue when implementing on \acrshort{fpga}.
\subsection{Efficient Model Design}
The size of the model can be reduced by changing the architectures of the models. As many approaches have been proposed, we focus our interest on architectures that target the embedded space. \newline \newline
%
SqueezeNet \cite{iandola_squeezenet_2016} uses an architecture similar to AlexNet and replaces all layers (except the first and last one) by \textbf{Fire Module}. The \textbf{Fire Module} is a building blocks where the convolution filter is composed of two layers. The first one is composed only of $1 \times 1$ filters and the second one is composed of $1 \times 1$ and $3 \times 3$ convolutions. We can reduce with this method the number of parameters of AlexNet from $240$MB to $4.8$MB. The number of parameters can even be reduced to $0.47$MB with no loss of accuracy from the baseline AlexNet method by applying Deep Compression \cite{han_deep_2016}. \newline \newline
%
NasNet \cite{zoph_learning_2018} uses a search method, \acrfull{nas}, to find good convolutional architectures on a dataset of interest. A controller recurrent neural network saloes child networks with different architecture. The learned architecture is flexible as it may be scaled in terms of computational cost. However the resulting network ends up very complex \cite{sandler_mobilenetv2_2019}.\newline \newline
%
MobileNet \cite{howard_mobilenets_2017} uses \acrshort{dsc}, described in section \ref{subs:dsc}, to build small and low latency models that can be matched to the design requirements. It sets also 2 hyper-parameters to set the model size and throughput: width multiplier $\alpha \in [1; 0[$, which reduces the number of input and output channel at each layer, and resolution multiplier $\rho \in [1; 0[$,  which reduces spatially the input and output \acrshort{fm} at each layer.\newline \newline
%
ShuffleNet \cite{zhang_shufflenet_2018} is a computation-efficient architecture designed for mobile devices with very limited computing power. It reduces computation cost while maintaining accuracy by using \textbf{pointwise group convolution} which reduces computation complexity of $1 \times 1$ convolution. It uses also \textbf{channel shufï¬‚e} on the channels such that \textbf{group convolutions} obtain information from different groups. Then more powerful structures can be build  with multiple group convolutional layer. \newline \newline
%
MobileNetV2 \cite{sandler_mobilenetv2_2019} is an improvement of MobileNet in terms of accuracy and does not require special operators. MobileNetV2 has already been developed at section \ref{subs:mbv2}. The architecture developed by this work has been developed on MobileNetV2 because of its simplicity and its state-of-the-art performance (see table \ref{tab:mbv2}).
%
\begin{table}
    \center
    \begin{tabular}{ | c | c | c c | c| }
        \hline \hline
        Network & Top 1 & Params & MAdds & CPU \\
        \hline \hline
        MobileNetV1 & 70.6 & 4.2M & 575M & 113ms \\
        ShuffleNet (1.5) & 71.5 & \textbf{3.4M} & 292M & - \\
        ShuffleNet (x2)  & 73.7 & 5.4M & 524M & - \\
        NasNet-A & 74.0 & 5.3M & 564M & 183ms \\
        \hline
        MobileNetV2 & \textbf{72.0} & \textbf{3.4M} & \textbf{300M} & \textbf{75ms} \\
        MobileNetV2 (1.4) & \textbf{74.7} & 6.9M & 585M & \textbf{143ms} \\
        \hline \hline
    \end{tabular}
    \caption{Performance on ImageNet, comparison for different networks \cite{sandler_mobilenetv2_2019}}
    \label{tab:mbv2}
\end{table}
\subsection{Pruning}

\subsection{Quantization}
The approach of quantization is to compress (reducing the bit-width) floating point parameters to fixed point low-precision parameters. The benefits of quantization, as enonced by \cite{joos_de_ter_beerst_accelerating_2019}, are a reduction of overfitting and an acceleration of computation because of the weights of smaller bit-size. For example, operations can be four 4 times faster when quantizing to 8 bits due to \acrshort{simd} optimisations. \newline \newline
%
There are two forms of quantization:
\begin{enumerate}
    \item \textbf{Fixed-point integers}: a number representation using bits is divided into two parts: the first part is dedicated to the integer part and the second one is dedicated to the fractional part of the number. It is easy to implement but it is important to consider the bit size of each part. It can be done by examination after the training. We can also, as for \cite{21,22}, choose a different range for each layer (doing it for every weight is not memory-efficient).
    \item \textbf{Inteval quantification}: we redefine weights to be between $[-\alpha; \alpha]$. We have to retrain a network before using it but we can achieve good results with a small number of bits.
\end{enumerate}
%
It was only recently that work are published on highly quantized \acrshort{nn} \cite{guo_survey_2018}, and using 16-bits or 8-bits quantization has shown only a small loss in accuracy without retraining \cite{abdelouahab_accelerating_2018}. The quantization of a \acrshort{nn} can be achieved through two different approaches:
\begin{enumerate}
    \item \textbf{Quantification of the weight parameters}: the most widely used. It can be done before or after training, but we have a better accuracy if we train during training.
    \item \textbf{Quantification through the activation layer}: we quantize the the output of the activation function;
\end{enumerate}
Unfortunately, not all existing network are friendly for quantization, like MobileNetV1. There is a accuracy gap against float point models (70.50\% vs 1.80\% using a 8-bit pipeline). However, works in \cite{sheng_quantization-friendly_2018} have shown that the source of accuracy drap was the design of the separable convolution core layer. They have therefore proposed a new quantization-friendly separable convolution core layer. Works on MobileNetV2 should then be done to verify thr fixed-point inference accuracy.
