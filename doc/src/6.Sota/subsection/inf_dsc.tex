\subsection{DSC}
%
%
Some works put their interest in the implementation of the \acrshort{dsc}. In particular, we analyse the work of \textcite{bai_cnn_2018, liu_fpga-based_2019}. They present an \acrshort{fpga} accelerator that implement both standard and depthwise separable convolution. Networks like MobileNetV2 uses the 2 convolutions, the first one for not loosing to much information, the second one to speed up the inference. They both use a a commonly used \acrshort{dl} architecture that is called a heterogeneous system: a \acrshort{cpu} which controls the memory accesses and an accelerator that computes the convolution. The accelerator is designed in such a way that it can perform each layer of the network.

The accelerator in each work has the same structure. The accelerator uses weights, input and output buffers. To use the same accelerator for both convolutions, they firt compute an element-wise multiplication followed by an adder tree. The pattern of additions depens. \textcite{bai_cnn_2018} divides depthwise and pointwise convolution. As the element-wise multiplication produces a cuboid, we only have to sum either in the spatial or channel axis, either all the cuboid pixels to perform the desired convolution. On his side, \textcite{liu_fpga-based_2019} performs both convolution using the same adder tree. The dataflow to feed the adder tree depends then on the convolution. However, to use the same structure for both convolution, it pads some inputs with 0 value for separable convolution (the separable convolution has less arithmetic operations).

To improve the throughout of the network, they use a ping-pong buffer for the weight. Instead of using one buffer that stores the data for convolution, they use two buffers. Alternatively, one fetches weights for the next tile while the other is used for convolution. An illustration is found in Figure \ref{fig:ping_pong_buffer}.

Moreover, \textcite{liu_fpga-based_2019} has used the roofline model as method for design space exploration.
%
%
\subsection{Pruning} \label{subsec:impl_prun}
%
%
When implementing a pruned \acrshort{cnn} on an \acrshort{fpga}, we must address the problem of sparsity to keep the parallelism and the performance of the \acrshort{fpga} \cite{zhu_efficient_2020}. As a regular data access pattern is assured by applying a structured pruning scheme, there are some source of inefficiency that an implementation must handle.

First, the \acrshort{pe}s must avoid performing computation involving 0 weights. \textcite{kang_accelerator-aware_2020} fetches $N_{par}$ data in the channel axis and the $N_{non-zero}$ weights corresponding to the fetched group. A mutliplexer is used to choose the pixels associated to non-zero weights. An improvement can be made by avoiding 0 value pixel. \textcite{zhu_efficient_2020} clock gated the computation involving pixels with 0 value to save energy.

Second, in order to reduce the storage utilization, the zero-weights in a kernel must be discarded. The weights must be encoded in such a way that we keep both values and index of the weights to reduce the overhead of computing the output address.  Usually, kernels are encoded in format derived from the \acrfull{csr} \cite{mao_exploring_2017}. For example, \cite{zhu_efficient_2020} modified the format to compress further and accelerate the inference.

Finally, the problem of \textbf{load-imballance} can arise if the number of non-pruned weights is different in each \acrshort{pe} \cite{kim_zena_2018}. Therefore some \acrshort{pe} can finish before an other one and it can lead to inefficiency. \textcite{zhu_efficient_2020, kang_accelerator-aware_2020} solved this by setting an uniform number of weight in each kernels or group of fetched weights.
