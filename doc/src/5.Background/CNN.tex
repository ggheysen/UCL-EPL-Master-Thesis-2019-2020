\chapter{Convolutional Neural Network} \label{chap:cnn}
\acrshort{cnn} is a type of \acrshort{nn} that is specialized in analyzing visual imagery and natural language processing. \acrshort{cnn}s are feedforward, sparsely connected \acrshort{nn}s, structured as a pipeline of layers \cite{abdelouahab_accelerating_2018}. It has shown superior performance in several competitions related to Computer Vision and Image Processing \cite{khan_survey_2020}. We explore, in this chapter, the different aspects of a \acrshort{cnn} and what optimizations can be made to reduce its size and its computational complexity.

The first section \ref{sec:layer} introduces the building blocks of a \acrshort{cnn}. We start from the first and most simple element, the perceptron. From this first step, we explore how to use it to make layers that can perform more complex functions. We also detail the other layers required to improve the efficiency of the model, such as pooling.

The second section \ref{sec:models} details how we can use the layers from Section \ref{sec:layer} to build an efficient network. We discuss state-of-art networks like AlexNet, VGG16, ResNet, and MobileNetV2.

The third section \ref{sec:train} focuses on the training of a \acrshort{cnn}. We briefly explain the back-propagation algorithm, which consists of a forward-propagation (Section \ref{subs:trainforward}) and a back-propagation (Section \ref{subs:trainbackward}).

In the fourth section, we explore the state-of-the-art approaches to reduce the arithmetic complexity and the hardware utilization of \acrshort{cnn} models. Section \ref{subsec:algopti} details how to efficiently handle and optimize the convolution operation on \acrshort{fpga}. On the other side, Section \ref{subsec:mdopti} goes over techniques to reduce the size of the model.
%
%
\section{Layers} \label{sec:layer}
A layer is a high-level building block in a \acrshort{dl} network. It is a set of operations, weights, and non-linear functions. \acrshort{cnn} is defined as a pipeline of layers and we can build a network by stacking layers. The output of one layer can, therefore, be used as input for the following layer or as the final output of the network. We begin our understanding of the layer theory in Section \ref{subs:perceptron} by explaining the perceptron, the simplest element in an \acrshort{nn}.
%
%
\input{src/5.Background/subsection/CNN_Perceptron}
%
\input{src/5.Background/subsection/CNN_acti}
%
\input{src/5.Background/subsection/CNN_FCN}
%
\input{src/5.Background/subsection/CNN_conv}
%
\input{src/5.Background/subsection/CNN_dsc}
%
\input{src/5.Background/subsection/CNN_pooling}
%
%
\section{CNN Models} \label{sec:models}
Once we have seen how to build a \acrshort{cnn}, we can now detail some of the well-known \acrshort{cnn} networks. They are an example of how to construct an efficient \acrshort{cnn}.

%
\input{src/5.Background/subsection/CNN_models}
%
%
\section{Training} \label{sec:train}
We have seen in the previous section what are the common building blocks in a \acrshort{cnn}. Now we explore how a network improves its performance at a task automatically. This process is called \textquote{learning}. Finding the correct methods to find the weights is an important aspect to have an efficient network.

The learning phase starts after designing the network. However, before the model can learn, we have to assign a value to the weight. The most common form is random initialization drawn from Gaussian distribution \cite{he_delving_2015}. The learning is affected by the initial values of the weight. If it is too small the network does not learn or if it is too large it might take a very long time to converge. To solve this, different initializations have been suggested: Xavier initialization \cite{glorot_understanding_2010} and He initialization \cite{he_delving_2015}.

Once the weights have been initialized, we can perform the back-propagation algorithm to improve the efficiency of the network. It is composed of two steps: the forward pass (section \ref{subs:trainforward}) and the backward pass (section \ref{subs:trainbackward}).
%
\input{src/5.Background/subsection/CNN_forward}
%
\input{src/5.Background/subsection/CNN_backward}
%
%
\section{CNN optimizations for FPGA}
%
%
We have seen in the previous sections how \acrshort{cnn} works and state-of-the-art models. But as said in section \ref{subs:mbv2}, the huge arithmetic complexity and hardware utilization is a problem when implementing the inference on mobile devices such as an \acrshort{fpga}. In the following sections, we detail approaches to handle those two issues on an \acrshort{fpga}. Section \ref{subsec:algopti} focuses on techniques to speed up the convolution operations and Section \ref{subsec:mdopti} reviews techniques to reduce the size of the \acrshort{cnn}.
%
\input{src/5.Background/subsection/opti_algo}
%
\input{src/5.Background/subsection/opti_md}


\fbox{\parbox{ \linewidth \fboxrule \fboxsep }{ \textbf{Conclusion about the CNN optimizations}:

\vspace{5mm}
We have seen in this chapter what is a \acrshort{cnn}, how to build, and train it. We have also seen that the most performing models have a huge computational complexity and memory utilization, which limit the implementation of such models on mobile platforms, such as \acrshort{fpga}. Different approaches can be explored such as fast convolutions algorithms. But they only reduce the computational complexity while increasing hardware utilization. Therefore, pruning and model designs seem to be promising optimizations because they both aim at reducing those problems. Therefore, this work focus on the use of both approaches to see if their gain can be combined. To show our results, we apply pruning on MobileNetV2. As quantizing is an orthogonal approach to pruning, we also use 16-bit fixed-point formats because the loss of accuracy can be controlled.
}}
