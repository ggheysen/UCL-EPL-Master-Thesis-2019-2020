\subsection{Back-propagation} \label{subs:trainbackward}
According to \textcite{ruder_overview_2017}, gradient descent optimization algorithms are the most common one used  to perform optimizations on \acrshort{nn}. These algorithms derive from the idea of gradient descent. As said in the previous section, gradient descent is a way to minimize the loss function in order to parametriz the parameters of the model (weight). Equation \eqref{eq:gd} defines the gradient descent algorithm, where $\eta$ is the learning rate, a positive scalar determining the size of the step in the direction minimizing the gradient \cite{ruder_overview_2017, goodfellow_deep_2016}. If we update the weight in the opposite direction of the gradient, we can reach a local minimum.
%
\begin{equation}
    \boldsymbol{w} = \boldsymbol{w} - \eta \frac{ \partial L( \boldsymbol{x}, \boldsymbol{w} ) }{\partial \boldsymbol{w}}
    \label{eq:gd}
\end{equation}

\textbf{The original gradient descent} or \textbf{batch gradient descent} computes the gradient using the whole dataset (the batch) \cite{ruder_overview_2017, matteucci_artificial_2019}. Equation \eqref{eq:gd-grad} is used to compute this batch gradient. However, this might be impossible to do in practice if the dataset is too large. Indeed, Indeed, the gradient of the whole dataset has to be computed for each update. Therefore, variations of this algorithm have been proposed to make the gradient descent practical.
%
\begin{equation}
    \frac{ \partial L( \boldsymbol{x}, \boldsymbol{w} ) }{\partial \boldsymbol{w}} = \frac{1}{N_{in}} \sum^{Nin}_{i = 0} \frac{ \partial L( x_i, \boldsymbol{w} ) }{\partial \boldsymbol{w}}
    \label{eq:gd-grad}
\end{equation}

\textbf{Stochastic gradient descent}, instead of using the entire dataset, performs the gradient descent algorithm on each training sample of the whole dataset \cite{ruder_overview_2017, matteucci_artificial_2019}. Equation \eqref{eq:sgd-grad} computes the stochastic gradient descent. It avoids the redundant computation of the batch gradient descent. It learns faster and can reach better local minima, however, it is complicated to find the global minimum.
%
\begin{equation}
    \frac{ \partial L( \boldsymbol{x}, \boldsymbol{w} ) }{\partial \boldsymbol{w}} = \frac{ \partial L( x_i, \boldsymbol{w} ) }{\partial \boldsymbol{w}}
    \label{eq:sgd-grad}
\end{equation}

\textbf{Mini-batch gradient descent} is a trade-off between the two previous approaches \cite{ruder_overview_2017, matteucci_artificial_2019}. It performs an update of the weights for every mini-batch of the whole dataset. Equation \eqref{eq:bgd-grad} computes the mini-batch gradient descent, where $N$ is the number of training samples. It shows a better convergence than stochastic gradient descent by reducing the variance and has less computations than batch gradient descent.
%
\begin{equation}
    \frac{ \partial L( \boldsymbol{x}, \boldsymbol{w} ) }{\partial \boldsymbol{w}} = \frac{1}{N} \sum^{N < N_{in}}_{i = 0} \frac{ \partial L( x_i, \boldsymbol{w} ) }{\partial \boldsymbol{w}}
    \label{eq:bgd-grad}
\end{equation}
