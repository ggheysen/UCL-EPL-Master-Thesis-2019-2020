\subsubsection{Perceptron} \label{subs:perceptron}
The perceptron is a computational model that was developed in 1957 by \textcite{rosenblatt_perceptron_1958}. This computational model is inspired by the neurons which are the computational units of the human brain. A neuron receives information and when its threshold is reached, this information is released and transmitted to other neurons (the neuron \textquote{fires}) \cite{rosenblatt_perceptron_1958, matteucci_artificial_2019}.

The computational model of a neuron in a human brain corresponds to the perceptron. The perceptron is composed of $N_{in}$ inputs $\boldsymbol{x} = \{ x_1, ... x_{N_{in}} \}$, $N_{in}$ weights $\boldsymbol{w}$ and a bias $b$. Figure \ref{fig:perceptron} illustrates the working principle of a perceptron. When it receives an input vector $\boldsymbol{x}$, a weighted sum of this vector is computed. If its result is higher than the threshold (controlled by $b$), the perceptron is \textquote{activated} and its output becomes a non-zero value. Otherwise, the perceptron outputs a zero value.
Equation \eqref{eq:perceptron} represents the mathematical formula of the perceptron where $h$ is the activation function of the formula developed in Equation \eqref{eq:step} \cite{matteucci_artificial_2019}.
%
\begin{equation}
    h ( \boldsymbol{x} | \boldsymbol{w}, b) = h \left( \sum^{N_{in}}_{i=1} x_i \cdot w_i + b \right) = h \left( \boldsymbol{w}^{T} \cdot \boldsymbol{x} + b \right)
    \label{eq:perceptron}
\end{equation}
%
\begin{equation}
    h ( \boldsymbol{w}^{T} \cdot \boldsymbol{x} + b) = \begin{cases} 1, & \mbox{if } \boldsymbol{w}^{T} \cdot \boldsymbol{x} + b > 0 \\ 0, & \mbox{Otherwise} \end{cases}
    \label{eq:step}
\end{equation}
%
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{perceptron.pdf}
    \caption{The Perceptron}
    \label{fig:perceptron}
\end{figure}
%
As the perceptron computes a weighted sum, the perceptron can learn a task of interest by modifying those weights. However, this model is limited by the functions it can achieve. Later section details how we can use multiple perceptrons to create a \textbf{fully-connected layer}, learning more complex functions.
