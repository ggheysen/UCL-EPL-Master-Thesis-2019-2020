\subsection{AlexNet}
%
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{alexnet.pdf}
    \caption{An illustration of the architecture of AlexNet \cite{krizhevsky_imagenet_2012}}
    \label{fig:alexnet}
\end{figure}
%
AlexNet is a well-known \acrshort{cnn} which was developed in 2012 by \textcite{krizhevsky_imagenet_2012}. It is a breakthrough in the deep \acrshort{cnn} field and this model has won the ImageNet competition. Indeed, Krizhevsky et al. used some parameters optimizations and made the model deeper in order to considerably improved the learning ability of the CNN \cite{khan_survey_2020}. It is made composed of 5 convolutional layers and 3 fully-connected layers, as seen in Figure \ref{fig:alexnet}. Each convolutional layer has a ReLU activation function and it uses pooling.
\subsection{VGG}
%
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{VGG.pdf}
    \caption{An illustration of the architecture of VGG16 \cite{simonyan_very_2015}}
    \label{fig:vgg}
\end{figure}
%
After the success of AlexNet in 2012, research has been made to reduce the computational complexity while keeping the accuracy. In 2014, VGG, a deeper variant of AlexNet has been introduced developed by \textcite{simonyan_very_2015}. It is composed of 5 groups of convolutional layers, where the number of layers depends on the version of VGG. It has won the localization and the second-place tracks in the ImageNet challenge in 2014 \cite{simonyan_very_2015}. An illustration is provided by Figure \ref{fig:vgg}. This level of depth has been possible thanks to the application of very small ($3 \times 3$) convolution kernels. They allow a larger receptive field with having fewer parameters and more non-linearities than a larger kernel. However, it has a high memory request. 100MB per image needs to be stored in all \acrshort{fm}s for the forward propagation \cite{matteucci_artificial_2019}.
\subsection{ResNet}
%
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{resnet.pdf}
    \caption{ResNet building block \cite{he_deep_2015}}
    \label{fig:resnet}
\end{figure}
%
\textcite{he_deep_2015} have developed ResNet in 2015. It is a very deep network which can contain from 50 to 1000 convolutional layers. It is composed of structures which are more complex and irregular than in the networks described previously. \textcite{he_deep_2015} have shown that an increase of the depth of the network does not mean an improvement in the performance. Indeed, after a certain amount of layers, a continuous of the depth leads to the degradation of the accuracy. This is not due to overfitting but because the deeper models are harder to optimize than shallower ones (vanishing gradient described in Section \ref{subs:trainbackward}) \cite{matteucci_artificial_2019}. However, deeper networks should at least have similar or better performance than shallower ones. Indeed, letâ€™s compare a deeper and a shallower network. If the deeper network is composed of the same layers than the shallower one and that the other added layers are just identity mapping, then the network should have the same performance than the shallower one. Therefore, a deeper network should not have worst performance than shallower ones \cite{matteucci_artificial_2019}.

To overcome this issue, \textcite{he_deep_2015} introduced an \textit{identity shortcut connection} which skips one or more layers and set weights to the identity, as can be seen in Figure \ref{fig:resnet}. These weights associated to the shortcut connection can be used to learn a residual $\mathcal{F}(x)$ in order to improve the solution. The performance of this very deep network allowed him to win the 2015 ILSVR for both localization and classification.
%
\subsection{MobileNetV2} \label{subs:mbv2}
%
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{mbnv2.pdf}
    \caption{inverted residual with linear bottleneck \cite{sandler_mobilenetv2_2019}}
    \label{fig:invreslinbot}
\end{figure}

According to \textcite{cheng_recent_2018}, the performance of \acrshort{cnn} in recent years have become outstanding. These improvements came at the cost of storage and computational complexity. It is not an issue for the \acrshort{cnn} training phase, because the GPU and CPU have also gained in computational units and memory. However, for the inference phase, the computational complexity and the storage requirements are way beyond the capabilities of most of the embedded applications and mobile devices such \acrshort{fpga} \cite{cheng_recent_2018}.
%
\begin{itemize}
    \item The enormous computational complexity of \acrshort{cnn}s makes it difficult to deploy on real-time applications and it consumes battery power.
    \item The large number of parameters of \acrshort{cnn}s consumes considerable storage and run-time memory.
\end{itemize}

In order to overcome this issue, \textcite{sandler_mobilenetv2_2019} have introduced a network called MobileNetV2 in 2019. It is specifically developed for constrained environments. First, the size and number of operations is decreased thanks to DSC (see Section \ref{subs:dsc}) and thanks to a new type of layer \textit{inverted residual with a linear bottleneck}, which can be observed on Figure \ref{fig:invreslinbot}. This layer is composed of a $1 \times 1$ convolution to expand the number of the input \acrshort{fm} channels and then followed by a \acrshort{dsc}. The intermediate increase in the number of channels is supposed to counterbalance the loss of information that occurred by the ReLU. They have also added a skip connection to build a network of great depth, and the last convolution has a linear activation function \cite{sandler_mobilenetv2_2019}.
